{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5163df96",
   "metadata": {},
   "source": [
    "# Pytorch for reinforcement learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe9f040b",
   "metadata": {},
   "source": [
    "Here is teh quick introduction to reinforcement learning with pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e21b5988",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pfrl\n",
    "import torch\n",
    "import torch.nn\n",
    "import gym\n",
    "import numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e1e1a7c",
   "metadata": {},
   "source": [
    "PFRL can be used for any problems if they are modeled as \"enviroments\". Open AI gym provides various kinds of benchmark environ ments and defined scommon interface among them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "043d949a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "observation space: Box([-4.8000002e+00 -3.4028235e+38 -4.1887903e-01 -3.4028235e+38], [4.8000002e+00 3.4028235e+38 4.1887903e-01 3.4028235e+38], (4,), float32)\n",
      "action space: Discrete(2)\n",
      "initial observation: [-0.04186687  0.02193018  0.04498008  0.01211728]\n",
      "next observation: [-0.04142827 -0.17380701  0.04522243  0.3186458 ]\n",
      "reward: 1.0\n",
      "done: False\n",
      "info: {}\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "print('observation space:', env.observation_space)\n",
    "print('action space:', env.action_space)\n",
    "\n",
    "obs = env.reset()\n",
    "print('initial observation:', obs)\n",
    "\n",
    "action = env.action_space.sample()\n",
    "obs, r, done, info = env.step(action)\n",
    "print('next observation:', obs)\n",
    "print('reward:', r)\n",
    "print('done:', done)\n",
    "print('info:', info)\n",
    "\n",
    "# Uncomment to open a GUI window rendering the current state of the environment\n",
    "# env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c77ece7",
   "metadata": {},
   "source": [
    "PFRL provides various agents, each of which implements a deep reinforcement learning aglrithm.\n",
    "\n",
    "Let's try to use DoubleDQN algorithm which is implemented by `pfrl.agents.DoubleDQN`. this algorithm trains a Q-function that receives an observation and returns an expected future return for each action that agent can take. You an define your Q-function as `torch.nn.Module` as below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c2a8c852",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QFunction(torch.nn.Module):\n",
    "    def __init__(self, obs_size, n_actions):\n",
    "        super().__init__()\n",
    "        self.l1 = torch.nn.Linear(obs_size, 50)\n",
    "        self.l2 = torch.nn.Linear(50, 50)\n",
    "        self.l3 = torch.nn.Linear(50, n_actions)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        h = x\n",
    "        h = torch.nn.functional.relu(self.l1(h))\n",
    "        h = torch.nn.functional.relu(self.l2(h))\n",
    "        h = self.l3(h)\n",
    "        \n",
    "        return pfrl.action_value.DiscreteActionValue(h)\n",
    "    \n",
    "obs_size = env.observation_space.low.size\n",
    "n_actions = env.action_space.n\n",
    "q_function = QFunction(obs_size, n_actions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6909720",
   "metadata": {},
   "source": [
    "`pfrl.q_fuctions.DiscrenteActionValueHead` is just a torch.nn.Module that packs ints input to `pfrl.action_value.DiscreteActionValue`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9018751",
   "metadata": {},
   "source": [
    "As usual in PyTorch, `torch.optim.Optimizer` is used to optimize a model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8a35e482",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Adam to optimize q_func. eps=1e-2 is for stability.\n",
    "optimizer = torch.optim.Adam(q_function.parameters(), eps=1e-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecdd64fc",
   "metadata": {},
   "source": [
    "# Create Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c989a788",
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = 0.9\n",
    "\n",
    "# use epsilon-greedy for exploration\n",
    "explorer = pfrl.explorers.ConstantEpsilonGreedy(epsilon=0.3, random_action_func= env.action_space.sample)\n",
    "\n",
    "# DQN uses experience replay\n",
    "replay_buffer = pfrl.replay_buffers.ReplayBuffer(capacity=10**6)\n",
    "\n",
    "\n",
    "# since the observations from CartPole-v0 is numpy.float64 \n",
    "# whie as pytorch only accepts numpy.float32 by default, specify\n",
    "# a converter as a feature extractor function phi\n",
    "phi = lambda x: x.astype(numpy.float32, copy=False)\n",
    "\n",
    "gpu = -1\n",
    "\n",
    "agent = pfrl.agents.DoubleDQN(\n",
    "    q_function ,\n",
    "    optimizer,\n",
    "    replay_buffer,\n",
    "    gamma,\n",
    "    explorer,\n",
    "    replay_start_size=500,\n",
    "    update_interval=1,\n",
    "    target_update_interval=100,\n",
    "    phi=phi,\n",
    "    gpu=gpu\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f820ab67",
   "metadata": {},
   "source": [
    "Now that you have an agent and an environment, it's time to start reinforcement learning!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aea0f43",
   "metadata": {},
   "source": [
    "During training, two methods of `agent` must be called: `agent.act` and `agent.observe` \n",
    "\n",
    "* `agent.act(obs)` takes the curernt observation as input and returns an exploraty action. Once the returned action is processed in env, \n",
    "\n",
    "* `agent.observe(obs, reeward, done, reset)` then observes the consequences\n",
    "\n",
    "- `obs` : next observation\n",
    "- `reward` : an immediate reward\n",
    "- `done` : a boolean value set to True if reached a terminal state\n",
    "- `reset` : a boolean value set to True if an episode is interrupted at a non-terminal state, typically by a time limit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e3d3845c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode :  10 R:  11.0\n",
      "episode :  20 R:  9.0\n",
      "episode :  30 R:  11.0\n",
      "episode :  40 R:  14.0\n",
      "episode :  50 R:  9.0\n",
      "episode :  [('average_q', 0.27249628), ('average_loss', 0.29977213240721645), ('cumulative_steps', 533), ('n_updates', 34), ('rlen', 533)]\n",
      "episode :  60 R:  8.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode :  70 R:  10.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode :  80 R:  12.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode :  90 R:  14.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode :  100 R:  10.0\n",
      "episode :  [('average_q', 5.164761), ('average_loss', 0.20260722177103163), ('cumulative_steps', 1236), ('n_updates', 737), ('rlen', 1236)]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode :  110 R:  18.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode :  120 R:  15.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode :  130 R:  60.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode :  140 R:  53.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode :  150 R:  110.0\n",
      "episode :  [('average_q', 9.387445), ('average_loss', 0.2593049126991536), ('cumulative_steps', 3781), ('n_updates', 3282), ('rlen', 3781)]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode :  160 R:  140.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode :  170 R:  200.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode :  180 R:  200.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode :  190 R:  193.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode :  200 R:  200.0\n",
      "episode :  [('average_q', 10.068809), ('average_loss', 0.11590959727182053), ('cumulative_steps', 11375), ('n_updates', 10876), ('rlen', 11375)]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode :  210 R:  200.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode :  220 R:  200.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode :  230 R:  200.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode :  240 R:  200.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode :  250 R:  200.0\n",
      "episode :  [('average_q', 10.005127), ('average_loss', 0.06472850646940059), ('cumulative_steps', 20689), ('n_updates', 20190), ('rlen', 20689)]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode :  260 R:  175.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode :  270 R:  155.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode :  280 R:  183.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode :  290 R:  200.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode :  300 R:  172.0\n",
      "episode :  [('average_q', 9.975841), ('average_loss', 0.09845643496839329), ('cumulative_steps', 29750), ('n_updates', 29251), ('rlen', 29750)]\n"
     ]
    }
   ],
   "source": [
    "n_episodes = 300\n",
    "max_episode_len = 200\n",
    "\n",
    "history = []\n",
    "for i in range(1 , n_episodes+1):\n",
    "    obs = env.reset()\n",
    "    R = 0\n",
    "    t = 0\n",
    "    \n",
    "    while True:\n",
    "        # Uncomment to watch the behavior in GUI window\n",
    "        \n",
    "        action = agent.act(obs)\n",
    "        obs, reward, done, _ = env.step(action)\n",
    "        R += reward\n",
    "        t += 1\n",
    "        \n",
    "        reset = t == max_episode_len\n",
    "        agent.observe(obs, reward, done, reset)\n",
    "        if done or reset:\n",
    "            history.append(R)\n",
    "            break\n",
    "        \n",
    "    \n",
    "    if i %10 == 0:\n",
    "        print('episode : ',i ,'R: ', R)\n",
    "        \n",
    "    if i % 50 == 0:\n",
    "        print('episode : ', agent.get_statistics())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d29377a5",
   "metadata": {},
   "source": [
    "Now you finished the training the Double DQN agent for 300 episodes. How good is th agent now? You can evaluate it using `with agent.eval_mode()` . Exploration such as epsilon-greedy is not used anymore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6c7b3e3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluation episode:  0 R:  193.0\n",
      "evaluation episode:  1 R:  175.0\n",
      "evaluation episode:  2 R:  182.0\n",
      "evaluation episode:  3 R:  200.0\n",
      "evaluation episode:  4 R:  185.0\n",
      "evaluation episode:  5 R:  200.0\n",
      "evaluation episode:  6 R:  193.0\n",
      "evaluation episode:  7 R:  181.0\n",
      "evaluation episode:  8 R:  195.0\n",
      "evaluation episode:  9 R:  200.0\n"
     ]
    }
   ],
   "source": [
    "with agent.eval_mode():\n",
    "    for i in range(10):\n",
    "        obs = env.reset()\n",
    "        R = 0\n",
    "        t = 0\n",
    "        while True:\n",
    "            action = agent.act(obs)\n",
    "            obs, r , done , _ = env.step(action)\n",
    "            \n",
    "            R += r\n",
    "            t += 1\n",
    "            \n",
    "            reset = t== 200\n",
    "            agent.observe(obs, r, done, reset)\n",
    "            if done or reset:\n",
    "                break\n",
    "        \n",
    "        print('evaluation episode: ', i, 'R: ', R)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77275f4f",
   "metadata": {},
   "source": [
    "# Finishing up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0c726221",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.save('agent')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee80f2e1",
   "metadata": {},
   "source": [
    "# Shortcut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9c2aece0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outdir:result step:200 episode:0 R:200.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "statistics:[('average_q', 10.050055), ('average_loss', 0.04763803150504828), ('cumulative_steps', 29950), ('n_updates', 29451), ('rlen', 29950)]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outdir:result step:400 episode:1 R:200.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "statistics:[('average_q', 10.003617), ('average_loss', 0.07018168530310503), ('cumulative_steps', 30150), ('n_updates', 29651), ('rlen', 30150)]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outdir:result step:600 episode:2 R:200.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "statistics:[('average_q', 9.944943), ('average_loss', 0.06969788812333717), ('cumulative_steps', 30350), ('n_updates', 29851), ('rlen', 30350)]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outdir:result step:764 episode:3 R:164.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "statistics:[('average_q', 10.058809), ('average_loss', 0.05239647082518786), ('cumulative_steps', 30514), ('n_updates', 30015), ('rlen', 30514)]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outdir:result step:957 episode:4 R:193.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "statistics:[('average_q', 9.991211), ('average_loss', 0.07847402240789961), ('cumulative_steps', 30707), ('n_updates', 30208), ('rlen', 30707)]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outdir:result step:1157 episode:5 R:200.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "statistics:[('average_q', 10.015636), ('average_loss', 0.0489872798131546), ('cumulative_steps', 30907), ('n_updates', 30408), ('rlen', 30907)]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluation episode 0 length:165 R:165.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluation episode 1 length:165 R:165.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluation episode 2 length:149 R:149.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluation episode 3 length:144 R:144.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluation episode 4 length:146 R:146.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluation episode 5 length:147 R:147.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluation episode 6 length:143 R:143.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluation episode 7 length:172 R:172.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluation episode 8 length:189 R:189.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluation episode 9 length:144 R:144.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best score is updated -3.4028235e+38 -> 156.4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved the agent to result/best\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outdir:result step:1279 episode:6 R:122.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "statistics:[('average_q', 10.050219), ('average_loss', 0.05810094685410149), ('cumulative_steps', 31029), ('n_updates', 30530), ('rlen', 31029)]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outdir:result step:1461 episode:7 R:182.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "statistics:[('average_q', 10.083727), ('average_loss', 0.044854263817542234), ('cumulative_steps', 31211), ('n_updates', 30712), ('rlen', 31211)]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outdir:result step:1639 episode:8 R:178.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "statistics:[('average_q', 10.096574), ('average_loss', 0.06585528045892715), ('cumulative_steps', 31389), ('n_updates', 30890), ('rlen', 31389)]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outdir:result step:1839 episode:9 R:200.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "statistics:[('average_q', 10.077509), ('average_loss', 0.053921366051072256), ('cumulative_steps', 31589), ('n_updates', 31090), ('rlen', 31589)]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outdir:result step:2000 episode:10 R:161.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "statistics:[('average_q', 10.080789), ('average_loss', 0.067765186370234), ('cumulative_steps', 31750), ('n_updates', 31251), ('rlen', 31750)]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluation episode 0 length:200 R:200.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluation episode 1 length:200 R:200.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluation episode 2 length:200 R:200.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluation episode 3 length:200 R:200.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluation episode 4 length:200 R:200.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluation episode 5 length:200 R:200.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluation episode 6 length:200 R:200.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluation episode 7 length:200 R:200.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluation episode 8 length:200 R:200.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluation episode 9 length:200 R:200.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best score is updated 156.4 -> 200.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved the agent to result/best\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved the agent to result/2000_finish\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(<pfrl.agents.double_dqn.DoubleDQN at 0x107c994c0>,\n",
       " [{'average_q': 10.015636,\n",
       "   'average_loss': 0.0489872798131546,\n",
       "   'cumulative_steps': 30907,\n",
       "   'n_updates': 30408,\n",
       "   'rlen': 30907,\n",
       "   'eval_score': 156.4},\n",
       "  {'average_q': 10.080789,\n",
       "   'average_loss': 0.067765186370234,\n",
       "   'cumulative_steps': 31750,\n",
       "   'n_updates': 31251,\n",
       "   'rlen': 31750,\n",
       "   'eval_score': 200.0}])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set up the logger to print info messages for understandability.\n",
    "import logging\n",
    "import sys\n",
    "logging.basicConfig(level=logging.INFO, stream=sys.stdout, format='')\n",
    "\n",
    "pfrl.experiments.train_agent_with_evaluation(\n",
    "    agent,\n",
    "    env,\n",
    "    steps=2000,           # Train the agent for 2000 steps\n",
    "    eval_n_steps=None,       # We evaluate for episodes, not time\n",
    "    eval_n_episodes=10,       # 10 episodes are sampled for each evaluation\n",
    "    train_max_episode_len=200,  # Maximum length of each episode\n",
    "    eval_interval=1000,   # Evaluate the agent after every 1000 steps\n",
    "    outdir='result',      # Save everything to 'result' directory\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efb164a4",
   "metadata": {},
   "source": [
    "# Rainbow DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "43f008b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pfrl.q_functions import DistributionalDuelingDQN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e785481e",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3169043061.py, line 13)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Input \u001b[0;32mIn [11]\u001b[0;36m\u001b[0m\n\u001b[0;31m    )print(q_func)\u001b[0m\n\u001b[0m     ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "n_atoms = 51\n",
    "v_max = 10\n",
    "v_min = -10\n",
    "\n",
    "q_func = q_functions.DistributionalFCStateQFunctionWithDiscreteAction(\n",
    "        obs_size,\n",
    "        n_actions,\n",
    "        n_atoms,\n",
    "        v_min,\n",
    "        v_max,\n",
    "        n_hidden_channels=args.n_hidden_channels,\n",
    "        n_hidden_layers=args.n_hidden_layers,\n",
    ")print(q_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9f090bc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}