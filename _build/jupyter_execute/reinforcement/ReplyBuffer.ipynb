{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a3171d0c",
   "metadata": {},
   "source": [
    "# Replay Buffer\n",
    "\n",
    "Replay Buffer to store trajectories of experience when executing a policy in an environment. During training, replay buffers are queried for a subset of the trajectories ( either a sequential subset or a sample ) to \"replay\" the agent's exeprience"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cb9d71f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "from tf_agents import specs\n",
    "from tf_agents.agents.dqn import dqn_agent\n",
    "from tf_agents.drivers import dynamic_step_driver\n",
    "from tf_agents.environments import suite_gym\n",
    "from tf_agents.environments import tf_py_environment\n",
    "from tf_agents.networks import q_network\n",
    "from tf_agents.replay_buffers import py_uniform_replay_buffer\n",
    "from tf_agents.replay_buffers import tf_uniform_replay_buffer\n",
    "from tf_agents.specs import tensor_spec\n",
    "from tf_agents.trajectories import time_step, Trajectory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e82fd747",
   "metadata": {},
   "source": [
    "# Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "342928b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "env_name = \"CartPole-v0\" # @param {type:\"string\"}\n",
    "num_iterations = 250 # @param {type:\"integer\"}\n",
    "collect_episodes_per_iteration = 2 # @param {type:\"integer\"}\n",
    "replay_buffer_capacity = 2000 # @param {type:\"integer\"}\n",
    "\n",
    "fc_layer_params = (100,)\n",
    "\n",
    "learning_rate = 1e-3 # @param {type:\"number\"}\n",
    "log_interval = 25 # @param {type:\"integer\"}\n",
    "num_eval_episodes = 10 # @param {type:\"integer\"}\n",
    "eval_interval = 50 # @param {type:\"integer\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c9718919",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_env = suite_gym.load(env_name)\n",
    "eval_env = suite_gym.load(env_name)\n",
    "\n",
    "train_tf_env = tf_py_environment.TFPyEnvironment(train_env)\n",
    "eval_tf_env = tf_py_environment.TFPyEnvironment(eval_env)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31923d07",
   "metadata": {},
   "source": [
    "It has two elements, `time_step_spec` and `action_spec`. These two will go to trajectory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e027e31c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action_spec: BoundedArraySpec(shape=(), dtype=dtype('int64'), name='action', minimum=0, maximum=1)\n",
      "TimeStep(\n",
      "{'discount': BoundedArraySpec(shape=(), dtype=dtype('float32'), name='discount', minimum=0.0, maximum=1.0),\n",
      " 'observation': BoundedArraySpec(shape=(4,), dtype=dtype('float32'), name='observation', minimum=[-4.8000002e+00 -3.4028235e+38 -4.1887903e-01 -3.4028235e+38], maximum=[4.8000002e+00 3.4028235e+38 4.1887903e-01 3.4028235e+38]),\n",
      " 'reward': ArraySpec(shape=(), dtype=dtype('float32'), name='reward'),\n",
      " 'step_type': ArraySpec(shape=(), dtype=dtype('int32'), name='step_type')})\n"
     ]
    }
   ],
   "source": [
    "print('action_spec:', train_env.action_spec())\n",
    "print(train_env.time_step_spec())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10c9bf15",
   "metadata": {},
   "source": [
    "We see that environment expects action of type `int64` and rturns `TimeSteps` where observation are a `float32` vector of length 4 and discount factor is `float32`. now let's try to take a fixed action `(1,)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aea1faf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TimeStep(\n",
      "{'discount': array(1., dtype=float32),\n",
      " 'observation': array([ 0.04045126, -0.03406752,  0.01079454,  0.00798312], dtype=float32),\n",
      " 'reward': array(0., dtype=float32),\n",
      " 'step_type': array(0, dtype=int32)})\n",
      "TimeStep(\n",
      "{'discount': array(1., dtype=float32),\n",
      " 'observation': array([ 0.03976991,  0.16089797,  0.0109542 , -0.28127456], dtype=float32),\n",
      " 'reward': array(1., dtype=float32),\n",
      " 'step_type': array(1, dtype=int32)})\n",
      "TimeStep(\n",
      "{'discount': array(1., dtype=float32),\n",
      " 'observation': array([ 0.04298786,  0.35586196,  0.00532871, -0.57048255], dtype=float32),\n",
      " 'reward': array(1., dtype=float32),\n",
      " 'step_type': array(1, dtype=int32)})\n",
      "TimeStep(\n",
      "{'discount': array(1., dtype=float32),\n",
      " 'observation': array([ 0.05010511,  0.5509088 , -0.00608094, -0.86148196], dtype=float32),\n",
      " 'reward': array(1., dtype=float32),\n",
      " 'step_type': array(1, dtype=int32)})\n",
      "TimeStep(\n",
      "{'discount': array(1., dtype=float32),\n",
      " 'observation': array([ 0.06112328,  0.746113  , -0.02331058, -1.1560707 ], dtype=float32),\n",
      " 'reward': array(1., dtype=float32),\n",
      " 'step_type': array(1, dtype=int32)})\n",
      "TimeStep(\n",
      "{'discount': array(1., dtype=float32),\n",
      " 'observation': array([ 0.07604554,  0.941531  , -0.04643199, -1.4559706 ], dtype=float32),\n",
      " 'reward': array(1., dtype=float32),\n",
      " 'step_type': array(1, dtype=int32)})\n",
      "TimeStep(\n",
      "{'discount': array(1., dtype=float32),\n",
      " 'observation': array([ 0.09487616,  1.137191  , -0.07555141, -1.7627906 ], dtype=float32),\n",
      " 'reward': array(1., dtype=float32),\n",
      " 'step_type': array(1, dtype=int32)})\n",
      "TimeStep(\n",
      "{'discount': array(1., dtype=float32),\n",
      " 'observation': array([ 0.11761998,  1.333082  , -0.11080722, -2.0779796 ], dtype=float32),\n",
      " 'reward': array(1., dtype=float32),\n",
      " 'step_type': array(1, dtype=int32)})\n",
      "TimeStep(\n",
      "{'discount': array(1., dtype=float32),\n",
      " 'observation': array([ 0.14428163,  1.5291388 , -0.1523668 , -2.402772  ], dtype=float32),\n",
      " 'reward': array(1., dtype=float32),\n",
      " 'step_type': array(1, dtype=int32)})\n",
      "TimeStep(\n",
      "{'discount': array(1., dtype=float32),\n",
      " 'observation': array([ 0.1748644 ,  1.7252268 , -0.20042224, -2.7381191 ], dtype=float32),\n",
      " 'reward': array(1., dtype=float32),\n",
      " 'step_type': array(1, dtype=int32)})\n",
      "TimeStep(\n",
      "{'discount': array(0., dtype=float32),\n",
      " 'observation': array([ 0.20936893,  1.9211224 , -0.25518462, -3.0846112 ], dtype=float32),\n",
      " 'reward': array(1., dtype=float32),\n",
      " 'step_type': array(2, dtype=int32)})\n"
     ]
    }
   ],
   "source": [
    "action = np.array(1, dtype=np.int32)\n",
    "time_step = train_env.reset()\n",
    "print(time_step)\n",
    "while not time_step.is_last():\n",
    "    time_step = train_env.step(action)\n",
    "    print(time_step)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2511e36e",
   "metadata": {},
   "source": [
    "# Store the transitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4b67480b",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_step = train_env.reset()\n",
    "rewards = []\n",
    "steps = []\n",
    "num_episodes = 5\n",
    "for _ in range(num_episodes):\n",
    "    episode_reward = 0\n",
    "    episode_steps = 0\n",
    "    while not time_step.is_last():\n",
    "        action = np.random.choice([0,1])\n",
    "        time_step = train_env.step(action)\n",
    "        episode_steps += 1\n",
    "        episode_reward += time_step.reward\n",
    "        \n",
    "    rewards.append(episode_reward)\n",
    "    steps.append(episode_steps)\n",
    "    time_step = train_env.reset()\n",
    "    \n",
    "num_steps = np.sum(steps)\n",
    "avg_length = np.mean(steps)\n",
    "avg_reward = np.mean(rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5297084d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_episodes: 5 num_steps: 184\n",
      "avg_length 36.8 avg_reward: 36.8\n"
     ]
    }
   ],
   "source": [
    "print('num_episodes:', num_episodes, 'num_steps:', num_steps)\n",
    "print('avg_length', avg_length, 'avg_reward:', avg_reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d3ab060",
   "metadata": {},
   "source": [
    "# Replay Buffers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41027632",
   "metadata": {},
   "source": [
    "`PyUniformReplayBuffers` can be used to store the episodes and convert to batch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da55d6c0",
   "metadata": {},
   "source": [
    "For most agents, `collect_data_spec` is a named tuple called `Trajectory`, containing the specs for observations, actions, rewards, and other items.\"\"\"\n",
    "\n",
    "agent.collect_data_spec\n",
    "\n",
    "agent.collect_data_spec._fields\n",
    "\n",
    "## Data Collection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1954263",
   "metadata": {},
   "source": [
    "The most important method is action(time_step) which maps a time_step containing an observation from the environment to a PolicyStep named tuple containing the following attributes:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3143cd88",
   "metadata": {},
   "source": [
    "* action: The action to be applied to the environment.\n",
    "* state: The state of the policy (e.g. RNN state) to be fed into the next call to action.\n",
    "* info: Optional side information such as action log probabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac7b92f2",
   "metadata": {},
   "source": [
    "Environment provides two spec \n",
    "* env.time_step_spec() \n",
    "* env.action_spec()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5c73477e",
   "metadata": {},
   "outputs": [],
   "source": [
    "replay_buffer_capacity = 1000*32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "abbae939",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tf_agents.agents import data_converter\n",
    "from tf_agents.specs import tensor_spec\n",
    "from tf_agents.trajectories import time_step as ts\n",
    "from tf_agents.trajectories import trajectory\n",
    "from tf_agents.utils import test_utils\n",
    "from tf_agents.trajectories import policy_step\n",
    "from tf_agents.specs import array_spec\n",
    "from tf_agents.utils import nest_utils\n",
    "\n",
    "from tf_agents.drivers import py_driver\n",
    "from tf_agents.policies import py_tf_eager_policy\n",
    "from tf_agents.replay_buffers import episodic_replay_buffer\n",
    "\n",
    "from tf_agents.drivers import dynamic_episode_driver\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bfdf831c",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_step_spec = train_env.time_step_spec()\n",
    "action_spec = policy_step.PolicyStep(train_env.action_spec())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "166cb990",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_spec = trajectory.from_transition(time_step_spec, action_spec, time_step_spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "938e17b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal device set to: Apple M1 Pro\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-25 13:56:22.967318: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2022-06-25 13:56:22.967635: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'as_list'\n  In call to configurable 'EpisodicReplayBuffer' (<class 'tf_agents.replay_buffers.episodic_replay_buffer.EpisodicReplayBuffer'>)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [12]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m py_replay_buffer \u001b[38;5;241m=\u001b[39m \u001b[43mepisodic_replay_buffer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEpisodicReplayBuffer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcapacity\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_spec\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_spec\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompleted_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m stateful_buffer \u001b[38;5;241m=\u001b[39m episodic_replay_buffer\u001b[38;5;241m.\u001b[39mStatefulEpisodicReplayBuffer(\n\u001b[1;32m      7\u001b[0m     py_replay_buffer,\n\u001b[1;32m      8\u001b[0m     num_episodes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[1;32m      9\u001b[0m )\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/gin/config.py:1605\u001b[0m, in \u001b[0;36m_make_gin_wrapper.<locals>.gin_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1603\u001b[0m scope_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m in scope \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(scope_str) \u001b[38;5;28;01mif\u001b[39;00m scope_str \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m   1604\u001b[0m err_str \u001b[38;5;241m=\u001b[39m err_str\u001b[38;5;241m.\u001b[39mformat(name, fn_or_cls, scope_info)\n\u001b[0;32m-> 1605\u001b[0m \u001b[43mutils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maugment_exception_message_and_reraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merr_str\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/gin/utils.py:41\u001b[0m, in \u001b[0;36maugment_exception_message_and_reraise\u001b[0;34m(exception, message)\u001b[0m\n\u001b[1;32m     39\u001b[0m proxy \u001b[38;5;241m=\u001b[39m ExceptionProxy()\n\u001b[1;32m     40\u001b[0m ExceptionProxy\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtype\u001b[39m(exception)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\n\u001b[0;32m---> 41\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m proxy\u001b[38;5;241m.\u001b[39mwith_traceback(exception\u001b[38;5;241m.\u001b[39m__traceback__) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/gin/config.py:1582\u001b[0m, in \u001b[0;36m_make_gin_wrapper.<locals>.gin_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1579\u001b[0m new_kwargs\u001b[38;5;241m.\u001b[39mupdate(kwargs)\n\u001b[1;32m   1581\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1582\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mnew_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mnew_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1583\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m   1584\u001b[0m   err_str \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/tf_agents/replay_buffers/episodic_replay_buffer.py:234\u001b[0m, in \u001b[0;36mEpisodicReplayBuffer.__init__\u001b[0;34m(self, data_spec, capacity, completed_only, buffer_size, name_prefix, device, seed, begin_episode_fn, end_episode_fn, dataset_drop_remainder, dataset_window_shift)\u001b[0m\n\u001b[1;32m    231\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_writes \u001b[38;5;241m=\u001b[39m common\u001b[38;5;241m.\u001b[39mcreate_variable(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnum_writes_counter\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    233\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_device):\n\u001b[0;32m--> 234\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data_table \u001b[38;5;241m=\u001b[39m \u001b[43mepisode_table_fn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    235\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data_spec\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_capacity\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_name_prefix\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    236\u001b[0m   \u001b[38;5;66;03m# The episode ids\u001b[39;00m\n\u001b[1;32m    237\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_id_table \u001b[38;5;241m=\u001b[39m table_fn(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_id_spec, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_capacity)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/tf_agents/replay_buffers/episodic_table.py:82\u001b[0m, in \u001b[0;36mEpisodicTable.__init__\u001b[0;34m(self, tensor_spec, capacity, name_prefix)\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_create_storage\u001b[39m(spec, slot_name):\n\u001b[1;32m     74\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mlookup\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mDenseHashTable(\n\u001b[1;32m     75\u001b[0m       key_dtype\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mint64,\n\u001b[1;32m     76\u001b[0m       value_dtype\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mvariant,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     79\u001b[0m       name\u001b[38;5;241m=\u001b[39mslot_name,\n\u001b[1;32m     80\u001b[0m       default_value\u001b[38;5;241m=\u001b[39m_empty_slot(spec))\n\u001b[0;32m---> 82\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_storage \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_structure\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_create_storage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_tensor_spec\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     83\u001b[0m \u001b[43m                                      \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_slots\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variables \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mnest\u001b[38;5;241m.\u001b[39mflatten(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_storage)\n\u001b[1;32m     85\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_slot2variable_map \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flattened_slots, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variables))\n",
      "File \u001b[0;32m~/miniforge3/envs/meta/lib/python3.9/site-packages/tensorflow/python/util/nest.py:916\u001b[0m, in \u001b[0;36mmap_structure\u001b[0;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[1;32m    912\u001b[0m flat_structure \u001b[38;5;241m=\u001b[39m (flatten(s, expand_composites) \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m structure)\n\u001b[1;32m    913\u001b[0m entries \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mflat_structure)\n\u001b[1;32m    915\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m pack_sequence_as(\n\u001b[0;32m--> 916\u001b[0m     structure[\u001b[38;5;241m0\u001b[39m], [func(\u001b[38;5;241m*\u001b[39mx) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m entries],\n\u001b[1;32m    917\u001b[0m     expand_composites\u001b[38;5;241m=\u001b[39mexpand_composites)\n",
      "File \u001b[0;32m~/miniforge3/envs/meta/lib/python3.9/site-packages/tensorflow/python/util/nest.py:916\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    912\u001b[0m flat_structure \u001b[38;5;241m=\u001b[39m (flatten(s, expand_composites) \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m structure)\n\u001b[1;32m    913\u001b[0m entries \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mflat_structure)\n\u001b[1;32m    915\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m pack_sequence_as(\n\u001b[0;32m--> 916\u001b[0m     structure[\u001b[38;5;241m0\u001b[39m], [\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m entries],\n\u001b[1;32m    917\u001b[0m     expand_composites\u001b[38;5;241m=\u001b[39mexpand_composites)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/tf_agents/replay_buffers/episodic_table.py:80\u001b[0m, in \u001b[0;36mEpisodicTable.__init__.<locals>._create_storage\u001b[0;34m(spec, slot_name)\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_create_storage\u001b[39m(spec, slot_name):\n\u001b[1;32m     74\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mlookup\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mDenseHashTable(\n\u001b[1;32m     75\u001b[0m       key_dtype\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mint64,\n\u001b[1;32m     76\u001b[0m       value_dtype\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mvariant,\n\u001b[1;32m     77\u001b[0m       empty_key\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m     78\u001b[0m       deleted_key\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[1;32m     79\u001b[0m       name\u001b[38;5;241m=\u001b[39mslot_name,\n\u001b[0;32m---> 80\u001b[0m       default_value\u001b[38;5;241m=\u001b[39m\u001b[43m_empty_slot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mspec\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/tf_agents/replay_buffers/episodic_table.py:35\u001b[0m, in \u001b[0;36m_empty_slot\u001b[0;34m(spec)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_empty_slot\u001b[39m(spec):\n\u001b[0;32m---> 35\u001b[0m   shape \u001b[38;5;241m=\u001b[39m [s \u001b[38;5;28;01mif\u001b[39;00m s \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m \u001b[43mspec\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mas_list\u001b[49m()]\n\u001b[1;32m     36\u001b[0m   shape \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mconvert_to_tensor(value\u001b[38;5;241m=\u001b[39mshape, dtype\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mint64, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mshape\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     37\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m list_ops\u001b[38;5;241m.\u001b[39mempty_tensor_list(shape, spec\u001b[38;5;241m.\u001b[39mdtype)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'as_list'\n  In call to configurable 'EpisodicReplayBuffer' (<class 'tf_agents.replay_buffers.episodic_replay_buffer.EpisodicReplayBuffer'>)"
     ]
    }
   ],
   "source": [
    "py_replay_buffer = episodic_replay_buffer.EpisodicReplayBuffer(\n",
    "    capacity=2,\n",
    "    data_spec=data_spec,\n",
    "    completed_only=True)\n",
    "\n",
    "stateful_buffer = episodic_replay_buffer.StatefulEpisodicReplayBuffer(\n",
    "    py_replay_buffer,\n",
    "    num_episodes=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "950cef4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_step(environment, policy, buffer, id, policy_state):\n",
    "  time_step = environment.current_time_step()\n",
    "  if policy_state:\n",
    "      action_step = policy.action(time_step, policy_state)\n",
    "      policy_state = action_step.state\n",
    "  else:\n",
    "      action_step = policy.action(time_step)\n",
    "\n",
    "  next_time_step = environment.step(action_step.action)\n",
    "  traj = trajectory.from_transition(time_step, action_step, next_time_step)\n",
    "\n",
    "  id_tensor = tf.constant(id, dtype=tf.int64)\n",
    "  buffer.add_batch(traj, id_tensor)\n",
    "  if time_step.is_last():\n",
    "      id[0] += 1\n",
    "\n",
    "  return policy_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3e0971d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_data(env, policy, buffer, steps, id, policy_state=()):\n",
    "  for _ in range(steps):\n",
    "    policy_state = collect_step(env, policy, buffer, id, policy_state)\n",
    "\n",
    "  return policy_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6360af3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "episode_id = [0]\n",
    "collect_data(train_env, random_policy, py_replay_buffer, 2 , episode_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "076e6ef5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4326af1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def collect_episode(environment, policy, num_episodes):\n",
    "\n",
    "  driver = py_driver.PyDriver(\n",
    "    environment,\n",
    "    py_tf_eager_policy.PyTFEagerPolicy(\n",
    "      policy, use_tf_function=True),\n",
    "    [py_replay_buffer.add_batch],\n",
    "    max_episodes=num_episodes)\n",
    "  initial_time_step = environment.reset()\n",
    "  driver.run(initial_time_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a39013f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "collect_episode(train_env, tf_agent.collect_policy, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2577db6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_episode(environment, policy, buffer=None, steps=2):\n",
    "    observers = [buffer.add_batch]\n",
    "    \n",
    "    driver = dynamic_episode_driver.DynamicEpisodeDriver(\n",
    "            environment, policy, observers,num_episodes=steps)\n",
    "    \n",
    "    final_time_step, policy_state = driver.run()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89a1740b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@test {\"skip\": true}\n",
    "def collect_step(environment, policy, buffer):\n",
    "  time_step = environment.current_time_step()\n",
    "  action_step = policy.action(time_step)\n",
    "  next_time_step = environment.step(action_step.action)\n",
    "  traj = trajectory.from_transition(time_step, action_step, next_time_step)\n",
    "\n",
    "  #print(traj)\n",
    "  # Add trajectory to the replay buffer\n",
    "  buffer.add_batch(nest_utils.batch_nested_array(traj))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce3b20f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tf_agents.policies import random_py_policy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27f3ff7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_policy = random_py_policy.RandomPyPolicy(train_env.time_step_spec(),\n",
    "                                                train_env.action_spec())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fee17a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_data(env, policy, buffer, steps):\n",
    "  for _ in range(steps):\n",
    "    collect_step(env, policy, buffer)\n",
    "\n",
    "collect_data(train_env, random_policy, py_replay_buffer, steps=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e67e4a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "collect_episode(train_tf_env, random_policy, py_replay_buffer, steps=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "827fb0f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = py_replay_buffer.as_dataset(\n",
    "    sample_batch_size=batch_size,\n",
    "    num_steps=n_step_update + 1).prefetch(3)\n",
    "\n",
    "iterator = iter(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55a7a892",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84b53fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_avg_return(environment, policy, num_episodes = 10):\n",
    "    total_return = 0.0\n",
    "    for _ in range(num_episodes):\n",
    "        time_step = environment.reset()\n",
    "        episode_return = 0.0\n",
    "        \n",
    "        while not time_step.is_last():\n",
    "            action_step = policy.action(time_step)\n",
    "            time_step = environment.step(action_step.action)\n",
    "            episode_return += time_step.reward\n",
    "            \n",
    "        total_return += episode_return\n",
    "        \n",
    "    avg_return = total_return / num_episodes\n",
    "    return avg_return.numpy()[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc74d587",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tf_env = tf_py_environment.TFPyEnvironment(train_env)\n",
    "eval_tf_env = tf_py_environment.TFPyEnvironment(eval_env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f8d30d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tf_agents.networks import actor_distribution_network\n",
    "from tf_agents.policies import py_tf_eager_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a5cf328",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fc_layer_params = (100,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f337269",
   "metadata": {},
   "outputs": [],
   "source": [
    "actor_net = actor_distribution_network.ActorDistributionNetwork(\n",
    "    train_tf_env.observation_spec(),\n",
    "    train_tf_env.action_spec(),\n",
    "    fc_layer_params=fc_layer_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d6ccd95",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tf_agents.agents.reinforce import reinforce_agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf0973f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "\n",
    "train_step_counter = tf.Variable(0)\n",
    "\n",
    "tf_agent = reinforce_agent.ReinforceAgent(\n",
    "    train_env.time_step_spec(),\n",
    "    train_env.action_spec(),\n",
    "    actor_network=actor_net,\n",
    "    optimizer=optimizer,\n",
    "    normalize_returns=True,\n",
    "    train_step_counter=train_step_counter)\n",
    "tf_agent.initialize()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8714b705",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_agent.train_step_counter.assign(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05853fde",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_return = compute_avg_return(eval_tf_env, tf_agent.policy, num_eval_episodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f73ece8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "returns = [avg_return]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92372114",
   "metadata": {},
   "outputs": [],
   "source": [
    "returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c2da24b",
   "metadata": {},
   "outputs": [],
   "source": [
    "collect_episode(train_tf_env, tf_agent.collect_policy, py_replay_buffer, steps=2)\n",
    "\n",
    "    \n",
    "iterator = iter(py_replay_buffer.as_dataset(sample_batch_size=1))\n",
    "trajectories  = next(iterator)\n",
    "print(trajectories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63d4e9b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "py_replay_buffer.clear()\n",
    "\n",
    "for _ in range(num_iterations):\n",
    "    # collect few episodes using collect_policy and save to replay buffer\n",
    "    collect_episode(train_tf_env, tf_agent.collect_policy, py_replay_buffer, steps=2)\n",
    "    \n",
    "    iterator = iter(py_replay_buffer.as_dataset(sample_batch_size=1))\n",
    "    trajectories  = next(iterator)\n",
    "    print(trajectories)\n",
    "    batched_exp = tf.nest.map_structure(\n",
    "        lambda t: tf.expand_dims(t, axis=0),\n",
    "        trajectories\n",
    "    )\n",
    "    \n",
    "    train_loss = tf_agent.train(batched_exp).loss\n",
    "    \n",
    "    py_replay_buffer.clear()\n",
    "    \n",
    "    step = tf_agent.train_step_counter.numpy()\n",
    "    \n",
    "    if step % log_interval == 0:\n",
    "        print('step == {0}: loss = {1}'.format(step, train_loss.loss))\n",
    "        \n",
    "    if step % eval_interval == 0:\n",
    "        avg_return = compute_avg_return(eval_tf_env, tf_agent.policy, num_eval_episodes)\n",
    "        print('step = {0}: Average return = {1}'.format(step, avg_return))\n",
    "        \n",
    "        returns.append(avg_return)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c73e1e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = range(0, num_iterations + 1, eval_interval)\n",
    "plt.plot(steps, returns)\n",
    "plt.ylabel('Average Return')\n",
    "plt.xlabel('Step')\n",
    "plt.ylim(top=250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eb9b0d8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}