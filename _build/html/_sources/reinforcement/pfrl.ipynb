{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5163df96",
   "metadata": {},
   "source": [
    "# Pytorch for reinforcement learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe9f040b",
   "metadata": {},
   "source": [
    "Here is teh quick introduction to reinforcement learning with pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e21b5988",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pfrl\n",
    "import torch\n",
    "import torch.nn\n",
    "import gym\n",
    "import numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e1e1a7c",
   "metadata": {},
   "source": [
    "PFRL can be used for any problems if they are modeled as \"enviroments\". Open AI gym provides various kinds of benchmark environ ments and defined scommon interface among them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "043d949a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "observation space: Box([-4.8000002e+00 -3.4028235e+38 -4.1887903e-01 -3.4028235e+38], [4.8000002e+00 3.4028235e+38 4.1887903e-01 3.4028235e+38], (4,), float32)\n",
      "action space: Discrete(2)\n",
      "initial observation: [-0.00416541 -0.01051919 -0.02126048  0.02009261]\n",
      "next observation: [-0.0043758   0.1849011  -0.02085862 -0.27922168]\n",
      "reward: 1.0\n",
      "done: False\n",
      "info: {}\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "print('observation space:', env.observation_space)\n",
    "print('action space:', env.action_space)\n",
    "\n",
    "obs = env.reset()\n",
    "print('initial observation:', obs)\n",
    "\n",
    "action = env.action_space.sample()\n",
    "obs, r, done, info = env.step(action)\n",
    "print('next observation:', obs)\n",
    "print('reward:', r)\n",
    "print('done:', done)\n",
    "print('info:', info)\n",
    "\n",
    "# Uncomment to open a GUI window rendering the current state of the environment\n",
    "# env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c77ece7",
   "metadata": {},
   "source": [
    "PFRL provides various agents, each of which implements a deep reinforcement learning aglrithm.\n",
    "\n",
    "Let's try to use DoubleDQN algorithm which is implemented by `pfrl.agents.DoubleDQN`. this algorithm trains a Q-function that receives an observation and returns an expected future return for each action that agent can take. You an define your Q-function as `torch.nn.Module` as below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c2a8c852",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QFunction(torch.nn.Module):\n",
    "    def __init__(self, obs_size, n_actions):\n",
    "        super().__init__()\n",
    "        self.l1 = torch.nn.Linear(obs_size, 50)\n",
    "        self.l2 = torch.nn.Linear(50, 50)\n",
    "        self.l3 = torch.nn.Linear(50, n_actions)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        h = x\n",
    "        h = torch.nn.functional.relu(self.l1(h))\n",
    "        h = torch.nn.functional.relu(self.l2(h))\n",
    "        h = self.l3(h)\n",
    "        \n",
    "        return pfrl.action_value.DiscreteActionValue(h)\n",
    "    \n",
    "obs_size = env.observation_space.low.size\n",
    "n_actions = env.action_space.n\n",
    "q_function = QFunction(obs_size, n_actions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6909720",
   "metadata": {},
   "source": [
    "`pfrl.q_fuctions.DiscrenteActionValueHead` is just a torch.nn.Module that packs ints input to `pfrl.action_value.DiscreteActionValue`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9018751",
   "metadata": {},
   "source": [
    "As usual in PyTorch, `torch.optim.Optimizer` is used to optimize a model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8a35e482",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Adam to optimize q_func. eps=1e-2 is for stability.\n",
    "optimizer = torch.optim.Adam(q_function.parameters(), eps=1e-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecdd64fc",
   "metadata": {},
   "source": [
    "# Create Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c989a788",
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = 0.9\n",
    "\n",
    "# use epsilon-greedy for exploration\n",
    "explorer = pfrl.explorers.ConstantEpsilonGreedy(epsilon=0.3, random_action_func= env.action_space.sample)\n",
    "\n",
    "# DQN uses experience replay\n",
    "replay_buffer = pfrl.replay_buffers.ReplayBuffer(capacity=10**6)\n",
    "\n",
    "\n",
    "# since the observations from CartPole-v0 is numpy.float64 \n",
    "# whie as pytorch only accepts numpy.float32 by default, specify\n",
    "# a converter as a feature extractor function phi\n",
    "phi = lambda x: x.astype(numpy.float32, copy=False)\n",
    "\n",
    "gpu = -1\n",
    "\n",
    "agent = pfrl.agents.DoubleDQN(\n",
    "    q_function ,\n",
    "    optimizer,\n",
    "    replay_buffer,\n",
    "    gamma,\n",
    "    explorer,\n",
    "    replay_start_size=500,\n",
    "    update_interval=1,\n",
    "    target_update_interval=100,\n",
    "    phi=phi,\n",
    "    gpu=gpu\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f820ab67",
   "metadata": {},
   "source": [
    "Now that you have an agent and an environment, it's time to start reinforcement learning!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aea0f43",
   "metadata": {},
   "source": [
    "During training, two methods of `agent` must be called: `agent.act` and `agent.observe` \n",
    "\n",
    "* `agent.act(obs)` takes the curernt observation as input and returns an exploraty action. Once the returned action is processed in env, \n",
    "\n",
    "* `agent.observe(obs, reeward, done, reset)` then observes the consequences\n",
    "\n",
    "- `obs` : next observation\n",
    "- `reward` : an immediate reward\n",
    "- `done` : a boolean value set to True if reached a terminal state\n",
    "- `reset` : a boolean value set to True if an episode is interrupted at a non-terminal state, typically by a time limit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e3d3845c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode :  10 R:  22.0\n",
      "episode :  20 R:  54.0\n",
      "episode :  30 R:  25.0\n",
      "episode :  40 R:  200.0\n",
      "episode :  50 R:  15.0\n",
      "episode :  [('average_q', 9.972574), ('average_loss', 0.07234133171266877), ('cumulative_steps', 32522), ('n_updates', 32023), ('rlen', 32522)]\n",
      "episode :  60 R:  200.0\n",
      "episode :  70 R:  165.0\n",
      "episode :  80 R:  200.0\n",
      "episode :  90 R:  117.0\n",
      "episode :  100 R:  22.0\n",
      "episode :  [('average_q', 9.882232), ('average_loss', 0.05194314862310421), ('cumulative_steps', 38233), ('n_updates', 37734), ('rlen', 38233)]\n",
      "episode :  110 R:  77.0\n",
      "episode :  120 R:  20.0\n",
      "episode :  130 R:  147.0\n",
      "episode :  140 R:  84.0\n",
      "episode :  150 R:  200.0\n",
      "episode :  [('average_q', 9.911912), ('average_loss', 0.03334822844073642), ('cumulative_steps', 43553), ('n_updates', 43054), ('rlen', 43553)]\n",
      "episode :  160 R:  73.0\n",
      "episode :  170 R:  106.0\n",
      "episode :  180 R:  32.0\n",
      "episode :  190 R:  128.0\n",
      "episode :  200 R:  94.0\n",
      "episode :  [('average_q', 9.770783), ('average_loss', 0.04287147892871872), ('cumulative_steps', 47850), ('n_updates', 47351), ('rlen', 47850)]\n",
      "episode :  210 R:  109.0\n",
      "episode :  220 R:  18.0\n",
      "episode :  230 R:  113.0\n",
      "episode :  240 R:  13.0\n",
      "episode :  250 R:  132.0\n",
      "episode :  [('average_q', 9.726655), ('average_loss', 0.05477403025899548), ('cumulative_steps', 51390), ('n_updates', 50891), ('rlen', 51390)]\n",
      "episode :  260 R:  53.0\n",
      "episode :  270 R:  34.0\n",
      "episode :  280 R:  55.0\n",
      "episode :  290 R:  19.0\n",
      "episode :  300 R:  19.0\n",
      "episode :  [('average_q', 9.882959), ('average_loss', 0.07241410697635729), ('cumulative_steps', 55687), ('n_updates', 55188), ('rlen', 55687)]\n"
     ]
    }
   ],
   "source": [
    "n_episodes = 300\n",
    "max_episode_len = 200\n",
    "\n",
    "history = []\n",
    "for i in range(1 , n_episodes+1):\n",
    "    obs = env.reset()\n",
    "    R = 0\n",
    "    t = 0\n",
    "    \n",
    "    while True:\n",
    "        # Uncomment to watch the behavior in GUI window\n",
    "        \n",
    "        action = agent.act(obs)\n",
    "        obs, reward, done, _ = env.step(action)\n",
    "        R += reward\n",
    "        t += 1\n",
    "        \n",
    "        reset = t == max_episode_len\n",
    "        agent.observe(obs, reward, done, reset)\n",
    "        if done or reset:\n",
    "            history.append(R)\n",
    "            break\n",
    "        \n",
    "    \n",
    "    if i %10 == 0:\n",
    "        print('episode : ',i ,'R: ', R)\n",
    "        \n",
    "    if i % 50 == 0:\n",
    "        print('episode : ', agent.get_statistics())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d29377a5",
   "metadata": {},
   "source": [
    "Now you finished the training the Double DQN agent for 300 episodes. How good is th agent now? You can evaluate it using `with agent.eval_mode()` . Exploration such as epsilon-greedy is not used anymore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6c7b3e3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluation episode:  0 R:  152.0\n",
      "evaluation episode:  1 R:  141.0\n",
      "evaluation episode:  2 R:  145.0\n",
      "evaluation episode:  3 R:  146.0\n",
      "evaluation episode:  4 R:  139.0\n",
      "evaluation episode:  5 R:  127.0\n",
      "evaluation episode:  6 R:  145.0\n",
      "evaluation episode:  7 R:  153.0\n",
      "evaluation episode:  8 R:  172.0\n",
      "evaluation episode:  9 R:  147.0\n"
     ]
    }
   ],
   "source": [
    "with agent.eval_mode():\n",
    "    for i in range(10):\n",
    "        obs = env.reset()\n",
    "        R = 0\n",
    "        t = 0\n",
    "        while True:\n",
    "            action = agent.act(obs)\n",
    "            obs, r , done , _ = env.step(action)\n",
    "            \n",
    "            R += r\n",
    "            t += 1\n",
    "            \n",
    "            reset = t== 200\n",
    "            agent.observe(obs, r, done, reset)\n",
    "            if done or reset:\n",
    "                break\n",
    "        \n",
    "        print('evaluation episode: ', i, 'R: ', R)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77275f4f",
   "metadata": {},
   "source": [
    "# Finishing up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0c726221",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.save('agent')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee80f2e1",
   "metadata": {},
   "source": [
    "# Shortcut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9c2aece0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outdir:result step:133 episode:0 R:133.0\n",
      "statistics:[('average_q', 9.835213), ('average_loss', 0.07010247893922497), ('cumulative_steps', 25557), ('n_updates', 25058), ('rlen', 25557)]\n",
      "outdir:result step:333 episode:1 R:200.0\n",
      "statistics:[('average_q', 9.901389), ('average_loss', 0.07568743751209694), ('cumulative_steps', 25757), ('n_updates', 25258), ('rlen', 25757)]\n",
      "outdir:result step:533 episode:2 R:200.0\n",
      "statistics:[('average_q', 9.913004), ('average_loss', 0.06816803898662328), ('cumulative_steps', 25957), ('n_updates', 25458), ('rlen', 25957)]\n",
      "outdir:result step:660 episode:3 R:127.0\n",
      "statistics:[('average_q', 9.853989), ('average_loss', 0.05339874850236811), ('cumulative_steps', 26084), ('n_updates', 25585), ('rlen', 26084)]\n",
      "outdir:result step:689 episode:4 R:29.0\n",
      "statistics:[('average_q', 9.882291), ('average_loss', 0.06482254713831935), ('cumulative_steps', 26113), ('n_updates', 25614), ('rlen', 26113)]\n",
      "outdir:result step:869 episode:5 R:180.0\n",
      "statistics:[('average_q', 9.903717), ('average_loss', 0.06423532638698816), ('cumulative_steps', 26293), ('n_updates', 25794), ('rlen', 26293)]\n",
      "outdir:result step:1063 episode:6 R:194.0\n",
      "statistics:[('average_q', 9.848642), ('average_loss', 0.054336254785303024), ('cumulative_steps', 26487), ('n_updates', 25988), ('rlen', 26487)]\n",
      "evaluation episode 0 length:110 R:110.0\n",
      "evaluation episode 1 length:110 R:110.0\n",
      "evaluation episode 2 length:119 R:119.0\n",
      "evaluation episode 3 length:118 R:118.0\n",
      "evaluation episode 4 length:123 R:123.0\n",
      "evaluation episode 5 length:110 R:110.0\n",
      "evaluation episode 6 length:125 R:125.0\n",
      "evaluation episode 7 length:115 R:115.0\n",
      "evaluation episode 8 length:108 R:108.0\n",
      "evaluation episode 9 length:110 R:110.0\n",
      "The best score is updated -3.4028235e+38 -> 114.8\n",
      "Saved the agent to result/best\n",
      "outdir:result step:1197 episode:7 R:134.0\n",
      "statistics:[('average_q', 9.8270035), ('average_loss', 0.07156646179500967), ('cumulative_steps', 26621), ('n_updates', 26122), ('rlen', 26621)]\n",
      "outdir:result step:1212 episode:8 R:15.0\n",
      "statistics:[('average_q', 9.85048), ('average_loss', 0.08106264195754193), ('cumulative_steps', 26636), ('n_updates', 26137), ('rlen', 26636)]\n",
      "outdir:result step:1382 episode:9 R:170.0\n",
      "statistics:[('average_q', 9.921342), ('average_loss', 0.03889757478493266), ('cumulative_steps', 26806), ('n_updates', 26307), ('rlen', 26806)]\n",
      "outdir:result step:1582 episode:10 R:200.0\n",
      "statistics:[('average_q', 9.955715), ('average_loss', 0.061626185168279335), ('cumulative_steps', 27006), ('n_updates', 26507), ('rlen', 27006)]\n",
      "outdir:result step:1782 episode:11 R:200.0\n",
      "statistics:[('average_q', 9.933704), ('average_loss', 0.048936019096290695), ('cumulative_steps', 27206), ('n_updates', 26707), ('rlen', 27206)]\n",
      "outdir:result step:1888 episode:12 R:106.0\n",
      "statistics:[('average_q', 9.872984), ('average_loss', 0.06574177889793646), ('cumulative_steps', 27312), ('n_updates', 26813), ('rlen', 27312)]\n",
      "outdir:result step:2000 episode:13 R:112.0\n",
      "statistics:[('average_q', 9.926972), ('average_loss', 0.04484743709035684), ('cumulative_steps', 27424), ('n_updates', 26925), ('rlen', 27424)]\n",
      "evaluation episode 0 length:169 R:169.0\n",
      "evaluation episode 1 length:172 R:172.0\n",
      "evaluation episode 2 length:147 R:147.0\n",
      "evaluation episode 3 length:155 R:155.0\n",
      "evaluation episode 4 length:170 R:170.0\n",
      "evaluation episode 5 length:169 R:169.0\n",
      "evaluation episode 6 length:157 R:157.0\n",
      "evaluation episode 7 length:169 R:169.0\n",
      "evaluation episode 8 length:182 R:182.0\n",
      "evaluation episode 9 length:158 R:158.0\n",
      "The best score is updated 114.8 -> 164.8\n",
      "Saved the agent to result/best\n",
      "Saved the agent to result/2000_finish\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(<pfrl.agents.double_dqn.DoubleDQN at 0x1551b6a00>,\n",
       " [{'average_q': 9.848642,\n",
       "   'average_loss': 0.054336254785303024,\n",
       "   'cumulative_steps': 26487,\n",
       "   'n_updates': 25988,\n",
       "   'rlen': 26487,\n",
       "   'eval_score': 114.8},\n",
       "  {'average_q': 9.926972,\n",
       "   'average_loss': 0.04484743709035684,\n",
       "   'cumulative_steps': 27424,\n",
       "   'n_updates': 26925,\n",
       "   'rlen': 27424,\n",
       "   'eval_score': 164.8}])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set up the logger to print info messages for understandability.\n",
    "import logging\n",
    "import sys\n",
    "logging.basicConfig(level=logging.INFO, stream=sys.stdout, format='')\n",
    "\n",
    "pfrl.experiments.train_agent_with_evaluation(\n",
    "    agent,\n",
    "    env,\n",
    "    steps=2000,           # Train the agent for 2000 steps\n",
    "    eval_n_steps=None,       # We evaluate for episodes, not time\n",
    "    eval_n_episodes=10,       # 10 episodes are sampled for each evaluation\n",
    "    train_max_episode_len=200,  # Maximum length of each episode\n",
    "    eval_interval=1000,   # Evaluate the agent after every 1000 steps\n",
    "    outdir='result',      # Save everything to 'result' directory\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efb164a4",
   "metadata": {},
   "source": [
    "# Rainbow DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "43f008b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pfrl.q_functions import DistributionalDuelingDQN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e785481e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DistributionalDuelingDQN(\n",
      "  (conv_layers): ModuleList(\n",
      "    (0): Conv2d(4, 32, kernel_size=(8, 8), stride=(4, 4))\n",
      "    (1): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2))\n",
      "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "  )\n",
      "  (main_stream): Linear(in_features=3136, out_features=1024, bias=True)\n",
      "  (a_stream): Linear(in_features=512, out_features=102, bias=True)\n",
      "  (v_stream): Linear(in_features=512, out_features=51, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "n_atoms = 51\n",
    "v_max = 10\n",
    "v_min = -10\n",
    "\n",
    "q_func = q_functions.DistributionalFCStateQFunctionWithDiscreteAction(\n",
    "        obs_size,\n",
    "        n_actions,\n",
    "        n_atoms,\n",
    "        v_min,\n",
    "        v_max,\n",
    "        n_hidden_channels=args.n_hidden_channels,\n",
    "        n_hidden_layers=args.n_hidden_layers,\n",
    ")print(q_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9f090bc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
