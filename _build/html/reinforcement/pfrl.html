
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>Pytorch for reinforcement learning &#8212; Data Science notebook</title>
    
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">
<link href="../_static/styles/pydata-sphinx-theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" href="../_static/styles/sphinx-book-theme.css?digest=62ba249389abaaa9ffc34bf36a076bdc1d65ee18" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/design-style.b7bb847fb20b106c3d81b95245e65545.min.css" />
    
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf">

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?digest=f31d14ad54b65d19161ba51d4ffff3a77ae00456"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="60">
<!-- Checkboxes to toggle the left sidebar -->
<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation" aria-label="Toggle navigation sidebar">
<label class="overlay overlay-navbar" for="__navigation">
    <div class="visually-hidden">Toggle navigation sidebar</div>
</label>
<!-- Checkboxes to toggle the in-page toc -->
<input type="checkbox" class="sidebar-toggle" name="__page-toc" id="__page-toc" aria-label="Toggle in-page Table of Contents">
<label class="overlay overlay-pagetoc" for="__page-toc">
    <div class="visually-hidden">Toggle in-page Table of Contents</div>
</label>
<!-- Headers at the top -->
<div class="announcement header-item noprint"></div>
<div class="header header-item noprint"></div>

    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<!-- Sidebar -->
<div class="bd-sidebar noprint" id="site-navigation">
    <div class="bd-sidebar__content">
        <div class="bd-sidebar__top"><div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="../_static/logo.png" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">Data Science notebook</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../intro.html">
                    Welcome to your Jupyter Book
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Tensorflow
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../tensorflow/01_index.html">
   Introduction
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Reinforcement learning
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../markdown.html">
   Introduction to Reinforcement
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../notebooks.html">
   Example
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../markdown-notebooks.html">
   Notebooks with MyST Markdown
  </a>
 </li>
</ul>

    </div>
</nav></div>
        <div class="bd-sidebar__bottom">
             <!-- To handle the deprecated key -->
            
            <div class="navbar_extra_footer">
            Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
            </div>
            
        </div>
    </div>
    <div id="rtd-footer-container"></div>
</div>


          


          
<!-- A tiny helper pixel to detect if we've scrolled -->
<div class="sbt-scroll-pixel-helper"></div>
<!-- Main content -->
<div class="col py-0 content-container">
    
    <div class="header-article row sticky-top noprint">
        



<div class="col py-1 d-flex header-article-main">
    <div class="header-article__left">
        
        <label for="__navigation"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="right"
title="Toggle navigation"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-bars"></i>
  </span>

</label>

        
    </div>
    <div class="header-article__right">
<div class="menu-dropdown menu-dropdown-launch-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Launch interactive content">
      <i class="fas fa-rocket"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="https://mybinder.org/v2/gh/executablebooks/jupyter-book/master?urlpath=tree/docs/reinforcement/pfrl.ipynb"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Launch on Binder"
>
  

<span class="headerbtn__icon-container">
  
    <img src="../_static/images/logo_binder.svg">
  </span>
<span class="headerbtn__text-container">Binder</span>
</a>

      </li>
      
    </ul>
  </div>
</div>

<button onclick="toggleFullScreen()"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="bottom"
title="Fullscreen mode"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>

<div class="menu-dropdown menu-dropdown-repository-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Source repositories">
      <i class="fab fa-github"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="https://github.com/executablebooks/jupyter-book"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Source repository"
>
  

<span class="headerbtn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="headerbtn__text-container">repository</span>
</a>

      </li>
      
      <li>
        <a href="https://github.com/executablebooks/jupyter-book/issues/new?title=Issue%20on%20page%20%2Freinforcement/pfrl.html&body=Your%20issue%20content%20here."
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Open an issue"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="headerbtn__text-container">open issue</span>
</a>

      </li>
      
    </ul>
  </div>
</div>

<div class="menu-dropdown menu-dropdown-download-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Download this page">
      <i class="fas fa-download"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="../_sources/reinforcement/pfrl.ipynb"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Download source file"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="headerbtn__text-container">.ipynb</span>
</a>

      </li>
      
      <li>
        
<button onclick="printPdf(this)"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="left"
title="Print to PDF"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="headerbtn__text-container">.pdf</span>
</button>

      </li>
      
    </ul>
  </div>
</div>
<label for="__page-toc"
  class="headerbtn headerbtn-page-toc"
  
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-list"></i>
  </span>

</label>

    </div>
</div>

<!-- Table of contents -->
<div class="col-md-3 bd-toc show noprint">
    <div class="tocsection onthispage pt-5 pb-3">
        <i class="fas fa-list"></i> Contents
    </div>
    <nav id="bd-toc-nav" aria-label="Page">
        <ul class="visible nav section-nav flex-column">
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#">
   Pytorch for reinforcement learning
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#create-agent">
   Create Agent
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#finishing-up">
   Finishing up
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#shortcut">
   Shortcut
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#rainbow-dqn">
   Rainbow DQN
  </a>
 </li>
</ul>

    </nav>
</div>
    </div>
    <div class="article row">
        <div class="col pl-md-3 pl-lg-5 content-container">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>Pytorch for reinforcement learning</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#">
   Pytorch for reinforcement learning
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#create-agent">
   Create Agent
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#finishing-up">
   Finishing up
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#shortcut">
   Shortcut
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#rainbow-dqn">
   Rainbow DQN
  </a>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            <main id="main-content" role="main">
                
              <div>
                
  <section class="tex2jax_ignore mathjax_ignore" id="pytorch-for-reinforcement-learning">
<h1>Pytorch for reinforcement learning<a class="headerlink" href="#pytorch-for-reinforcement-learning" title="Permalink to this headline">#</a></h1>
<p>Here is teh quick introduction to reinforcement learning with pytorch</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pfrl</span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span>
<span class="kn">import</span> <span class="nn">gym</span>
<span class="kn">import</span> <span class="nn">numpy</span>
</pre></div>
</div>
</div>
</div>
<p>PFRL can be used for any problems if they are modeled as “enviroments”. Open AI gym provides various kinds of benchmark environ ments and defined scommon interface among them.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">env</span> <span class="o">=</span> <span class="n">gym</span><span class="o">.</span><span class="n">make</span><span class="p">(</span><span class="s1">&#39;CartPole-v0&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;observation space:&#39;</span><span class="p">,</span> <span class="n">env</span><span class="o">.</span><span class="n">observation_space</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;action space:&#39;</span><span class="p">,</span> <span class="n">env</span><span class="o">.</span><span class="n">action_space</span><span class="p">)</span>

<span class="n">obs</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">reset</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;initial observation:&#39;</span><span class="p">,</span> <span class="n">obs</span><span class="p">)</span>

<span class="n">action</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">action_space</span><span class="o">.</span><span class="n">sample</span><span class="p">()</span>
<span class="n">obs</span><span class="p">,</span> <span class="n">r</span><span class="p">,</span> <span class="n">done</span><span class="p">,</span> <span class="n">info</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">action</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;next observation:&#39;</span><span class="p">,</span> <span class="n">obs</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;reward:&#39;</span><span class="p">,</span> <span class="n">r</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;done:&#39;</span><span class="p">,</span> <span class="n">done</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;info:&#39;</span><span class="p">,</span> <span class="n">info</span><span class="p">)</span>

<span class="c1"># Uncomment to open a GUI window rendering the current state of the environment</span>
<span class="c1"># env.render()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>observation space: Box([-4.8000002e+00 -3.4028235e+38 -4.1887903e-01 -3.4028235e+38], [4.8000002e+00 3.4028235e+38 4.1887903e-01 3.4028235e+38], (4,), float32)
action space: Discrete(2)
initial observation: [-0.04186687  0.02193018  0.04498008  0.01211728]
next observation: [-0.04142827 -0.17380701  0.04522243  0.3186458 ]
reward: 1.0
done: False
info: {}
</pre></div>
</div>
</div>
</div>
<p>PFRL provides various agents, each of which implements a deep reinforcement learning aglrithm.</p>
<p>Let’s try to use DoubleDQN algorithm which is implemented by <code class="docutils literal notranslate"><span class="pre">pfrl.agents.DoubleDQN</span></code>. this algorithm trains a Q-function that receives an observation and returns an expected future return for each action that agent can take. You an define your Q-function as <code class="docutils literal notranslate"><span class="pre">torch.nn.Module</span></code> as below.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">QFunction</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">obs_size</span><span class="p">,</span> <span class="n">n_actions</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">l1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">obs_size</span><span class="p">,</span> <span class="mi">50</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">l2</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">50</span><span class="p">,</span> <span class="mi">50</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">l3</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">50</span><span class="p">,</span> <span class="n">n_actions</span><span class="p">)</span>
        
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">h</span> <span class="o">=</span> <span class="n">x</span>
        <span class="n">h</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">l1</span><span class="p">(</span><span class="n">h</span><span class="p">))</span>
        <span class="n">h</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">l2</span><span class="p">(</span><span class="n">h</span><span class="p">))</span>
        <span class="n">h</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">l3</span><span class="p">(</span><span class="n">h</span><span class="p">)</span>
        
        <span class="k">return</span> <span class="n">pfrl</span><span class="o">.</span><span class="n">action_value</span><span class="o">.</span><span class="n">DiscreteActionValue</span><span class="p">(</span><span class="n">h</span><span class="p">)</span>
    
<span class="n">obs_size</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">observation_space</span><span class="o">.</span><span class="n">low</span><span class="o">.</span><span class="n">size</span>
<span class="n">n_actions</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">action_space</span><span class="o">.</span><span class="n">n</span>
<span class="n">q_function</span> <span class="o">=</span> <span class="n">QFunction</span><span class="p">(</span><span class="n">obs_size</span><span class="p">,</span> <span class="n">n_actions</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p><code class="docutils literal notranslate"><span class="pre">pfrl.q_fuctions.DiscrenteActionValueHead</span></code> is just a torch.nn.Module that packs ints input to <code class="docutils literal notranslate"><span class="pre">pfrl.action_value.DiscreteActionValue</span></code></p>
<p>As usual in PyTorch, <code class="docutils literal notranslate"><span class="pre">torch.optim.Optimizer</span></code> is used to optimize a model</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Use Adam to optimize q_func. eps=1e-2 is for stability.</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">q_function</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-2</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="create-agent">
<h1>Create Agent<a class="headerlink" href="#create-agent" title="Permalink to this headline">#</a></h1>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">gamma</span> <span class="o">=</span> <span class="mf">0.9</span>

<span class="c1"># use epsilon-greedy for exploration</span>
<span class="n">explorer</span> <span class="o">=</span> <span class="n">pfrl</span><span class="o">.</span><span class="n">explorers</span><span class="o">.</span><span class="n">ConstantEpsilonGreedy</span><span class="p">(</span><span class="n">epsilon</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span> <span class="n">random_action_func</span><span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">action_space</span><span class="o">.</span><span class="n">sample</span><span class="p">)</span>

<span class="c1"># DQN uses experience replay</span>
<span class="n">replay_buffer</span> <span class="o">=</span> <span class="n">pfrl</span><span class="o">.</span><span class="n">replay_buffers</span><span class="o">.</span><span class="n">ReplayBuffer</span><span class="p">(</span><span class="n">capacity</span><span class="o">=</span><span class="mi">10</span><span class="o">**</span><span class="mi">6</span><span class="p">)</span>


<span class="c1"># since the observations from CartPole-v0 is numpy.float64 </span>
<span class="c1"># whie as pytorch only accepts numpy.float32 by default, specify</span>
<span class="c1"># a converter as a feature extractor function phi</span>
<span class="n">phi</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">numpy</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">copy</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

<span class="n">gpu</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span>

<span class="n">agent</span> <span class="o">=</span> <span class="n">pfrl</span><span class="o">.</span><span class="n">agents</span><span class="o">.</span><span class="n">DoubleDQN</span><span class="p">(</span>
    <span class="n">q_function</span> <span class="p">,</span>
    <span class="n">optimizer</span><span class="p">,</span>
    <span class="n">replay_buffer</span><span class="p">,</span>
    <span class="n">gamma</span><span class="p">,</span>
    <span class="n">explorer</span><span class="p">,</span>
    <span class="n">replay_start_size</span><span class="o">=</span><span class="mi">500</span><span class="p">,</span>
    <span class="n">update_interval</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
    <span class="n">target_update_interval</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span>
    <span class="n">phi</span><span class="o">=</span><span class="n">phi</span><span class="p">,</span>
    <span class="n">gpu</span><span class="o">=</span><span class="n">gpu</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Now that you have an agent and an environment, it’s time to start reinforcement learning!</p>
<p>During training, two methods of <code class="docutils literal notranslate"><span class="pre">agent</span></code> must be called: <code class="docutils literal notranslate"><span class="pre">agent.act</span></code> and <code class="docutils literal notranslate"><span class="pre">agent.observe</span></code></p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">agent.act(obs)</span></code> takes the curernt observation as input and returns an exploraty action. Once the returned action is processed in env,</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">agent.observe(obs,</span> <span class="pre">reeward,</span> <span class="pre">done,</span> <span class="pre">reset)</span></code> then observes the consequences</p></li>
</ul>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">obs</span></code> : next observation</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">reward</span></code> : an immediate reward</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">done</span></code> : a boolean value set to True if reached a terminal state</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">reset</span></code> : a boolean value set to True if an episode is interrupted at a non-terminal state, typically by a time limit</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">n_episodes</span> <span class="o">=</span> <span class="mi">300</span>
<span class="n">max_episode_len</span> <span class="o">=</span> <span class="mi">200</span>

<span class="n">history</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span> <span class="p">,</span> <span class="n">n_episodes</span><span class="o">+</span><span class="mi">1</span><span class="p">):</span>
    <span class="n">obs</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">reset</span><span class="p">()</span>
    <span class="n">R</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">t</span> <span class="o">=</span> <span class="mi">0</span>
    
    <span class="k">while</span> <span class="kc">True</span><span class="p">:</span>
        <span class="c1"># Uncomment to watch the behavior in GUI window</span>
        
        <span class="n">action</span> <span class="o">=</span> <span class="n">agent</span><span class="o">.</span><span class="n">act</span><span class="p">(</span><span class="n">obs</span><span class="p">)</span>
        <span class="n">obs</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">done</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">action</span><span class="p">)</span>
        <span class="n">R</span> <span class="o">+=</span> <span class="n">reward</span>
        <span class="n">t</span> <span class="o">+=</span> <span class="mi">1</span>
        
        <span class="n">reset</span> <span class="o">=</span> <span class="n">t</span> <span class="o">==</span> <span class="n">max_episode_len</span>
        <span class="n">agent</span><span class="o">.</span><span class="n">observe</span><span class="p">(</span><span class="n">obs</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">done</span><span class="p">,</span> <span class="n">reset</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">done</span> <span class="ow">or</span> <span class="n">reset</span><span class="p">:</span>
            <span class="n">history</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">R</span><span class="p">)</span>
            <span class="k">break</span>
        
    
    <span class="k">if</span> <span class="n">i</span> <span class="o">%</span><span class="k">10</span> == 0:
        <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;episode : &#39;</span><span class="p">,</span><span class="n">i</span> <span class="p">,</span><span class="s1">&#39;R: &#39;</span><span class="p">,</span> <span class="n">R</span><span class="p">)</span>
        
    <span class="k">if</span> <span class="n">i</span> <span class="o">%</span> <span class="mi">50</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;episode : &#39;</span><span class="p">,</span> <span class="n">agent</span><span class="o">.</span><span class="n">get_statistics</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>episode :  10 R:  11.0
episode :  20 R:  9.0
episode :  30 R:  11.0
episode :  40 R:  14.0
episode :  50 R:  9.0
episode :  [(&#39;average_q&#39;, 0.27249628), (&#39;average_loss&#39;, 0.29977213240721645), (&#39;cumulative_steps&#39;, 533), (&#39;n_updates&#39;, 34), (&#39;rlen&#39;, 533)]
episode :  60 R:  8.0
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>episode :  70 R:  10.0
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>episode :  80 R:  12.0
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>episode :  90 R:  14.0
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>episode :  100 R:  10.0
episode :  [(&#39;average_q&#39;, 5.164761), (&#39;average_loss&#39;, 0.20260722177103163), (&#39;cumulative_steps&#39;, 1236), (&#39;n_updates&#39;, 737), (&#39;rlen&#39;, 1236)]
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>episode :  110 R:  18.0
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>episode :  120 R:  15.0
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>episode :  130 R:  60.0
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>episode :  140 R:  53.0
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>episode :  150 R:  110.0
episode :  [(&#39;average_q&#39;, 9.387445), (&#39;average_loss&#39;, 0.2593049126991536), (&#39;cumulative_steps&#39;, 3781), (&#39;n_updates&#39;, 3282), (&#39;rlen&#39;, 3781)]
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>episode :  160 R:  140.0
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>episode :  170 R:  200.0
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>episode :  180 R:  200.0
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>episode :  190 R:  193.0
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>episode :  200 R:  200.0
episode :  [(&#39;average_q&#39;, 10.068809), (&#39;average_loss&#39;, 0.11590959727182053), (&#39;cumulative_steps&#39;, 11375), (&#39;n_updates&#39;, 10876), (&#39;rlen&#39;, 11375)]
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>episode :  210 R:  200.0
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>episode :  220 R:  200.0
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>episode :  230 R:  200.0
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>episode :  240 R:  200.0
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>episode :  250 R:  200.0
episode :  [(&#39;average_q&#39;, 10.005127), (&#39;average_loss&#39;, 0.06472850646940059), (&#39;cumulative_steps&#39;, 20689), (&#39;n_updates&#39;, 20190), (&#39;rlen&#39;, 20689)]
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>episode :  260 R:  175.0
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>episode :  270 R:  155.0
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>episode :  280 R:  183.0
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>episode :  290 R:  200.0
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>episode :  300 R:  172.0
episode :  [(&#39;average_q&#39;, 9.975841), (&#39;average_loss&#39;, 0.09845643496839329), (&#39;cumulative_steps&#39;, 29750), (&#39;n_updates&#39;, 29251), (&#39;rlen&#39;, 29750)]
</pre></div>
</div>
</div>
</div>
<p>Now you finished the training the Double DQN agent for 300 episodes. How good is th agent now? You can evaluate it using <code class="docutils literal notranslate"><span class="pre">with</span> <span class="pre">agent.eval_mode()</span></code> . Exploration such as epsilon-greedy is not used anymore.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">with</span> <span class="n">agent</span><span class="o">.</span><span class="n">eval_mode</span><span class="p">():</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">):</span>
        <span class="n">obs</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">reset</span><span class="p">()</span>
        <span class="n">R</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="n">t</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">while</span> <span class="kc">True</span><span class="p">:</span>
            <span class="n">action</span> <span class="o">=</span> <span class="n">agent</span><span class="o">.</span><span class="n">act</span><span class="p">(</span><span class="n">obs</span><span class="p">)</span>
            <span class="n">obs</span><span class="p">,</span> <span class="n">r</span> <span class="p">,</span> <span class="n">done</span> <span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">action</span><span class="p">)</span>
            
            <span class="n">R</span> <span class="o">+=</span> <span class="n">r</span>
            <span class="n">t</span> <span class="o">+=</span> <span class="mi">1</span>
            
            <span class="n">reset</span> <span class="o">=</span> <span class="n">t</span><span class="o">==</span> <span class="mi">200</span>
            <span class="n">agent</span><span class="o">.</span><span class="n">observe</span><span class="p">(</span><span class="n">obs</span><span class="p">,</span> <span class="n">r</span><span class="p">,</span> <span class="n">done</span><span class="p">,</span> <span class="n">reset</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">done</span> <span class="ow">or</span> <span class="n">reset</span><span class="p">:</span>
                <span class="k">break</span>
        
        <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;evaluation episode: &#39;</span><span class="p">,</span> <span class="n">i</span><span class="p">,</span> <span class="s1">&#39;R: &#39;</span><span class="p">,</span> <span class="n">R</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>evaluation episode:  0 R:  193.0
evaluation episode:  1 R:  175.0
evaluation episode:  2 R:  182.0
evaluation episode:  3 R:  200.0
evaluation episode:  4 R:  185.0
evaluation episode:  5 R:  200.0
evaluation episode:  6 R:  193.0
evaluation episode:  7 R:  181.0
evaluation episode:  8 R:  195.0
evaluation episode:  9 R:  200.0
</pre></div>
</div>
</div>
</div>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="finishing-up">
<h1>Finishing up<a class="headerlink" href="#finishing-up" title="Permalink to this headline">#</a></h1>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">agent</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="s1">&#39;agent&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="shortcut">
<h1>Shortcut<a class="headerlink" href="#shortcut" title="Permalink to this headline">#</a></h1>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Set up the logger to print info messages for understandability.</span>
<span class="kn">import</span> <span class="nn">logging</span>
<span class="kn">import</span> <span class="nn">sys</span>
<span class="n">logging</span><span class="o">.</span><span class="n">basicConfig</span><span class="p">(</span><span class="n">level</span><span class="o">=</span><span class="n">logging</span><span class="o">.</span><span class="n">INFO</span><span class="p">,</span> <span class="n">stream</span><span class="o">=</span><span class="n">sys</span><span class="o">.</span><span class="n">stdout</span><span class="p">,</span> <span class="nb">format</span><span class="o">=</span><span class="s1">&#39;&#39;</span><span class="p">)</span>

<span class="n">pfrl</span><span class="o">.</span><span class="n">experiments</span><span class="o">.</span><span class="n">train_agent_with_evaluation</span><span class="p">(</span>
    <span class="n">agent</span><span class="p">,</span>
    <span class="n">env</span><span class="p">,</span>
    <span class="n">steps</span><span class="o">=</span><span class="mi">2000</span><span class="p">,</span>           <span class="c1"># Train the agent for 2000 steps</span>
    <span class="n">eval_n_steps</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>       <span class="c1"># We evaluate for episodes, not time</span>
    <span class="n">eval_n_episodes</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>       <span class="c1"># 10 episodes are sampled for each evaluation</span>
    <span class="n">train_max_episode_len</span><span class="o">=</span><span class="mi">200</span><span class="p">,</span>  <span class="c1"># Maximum length of each episode</span>
    <span class="n">eval_interval</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span>   <span class="c1"># Evaluate the agent after every 1000 steps</span>
    <span class="n">outdir</span><span class="o">=</span><span class="s1">&#39;result&#39;</span><span class="p">,</span>      <span class="c1"># Save everything to &#39;result&#39; directory</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>outdir:result step:200 episode:0 R:200.0
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>statistics:[(&#39;average_q&#39;, 10.050055), (&#39;average_loss&#39;, 0.04763803150504828), (&#39;cumulative_steps&#39;, 29950), (&#39;n_updates&#39;, 29451), (&#39;rlen&#39;, 29950)]
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>outdir:result step:400 episode:1 R:200.0
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>statistics:[(&#39;average_q&#39;, 10.003617), (&#39;average_loss&#39;, 0.07018168530310503), (&#39;cumulative_steps&#39;, 30150), (&#39;n_updates&#39;, 29651), (&#39;rlen&#39;, 30150)]
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>outdir:result step:600 episode:2 R:200.0
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>statistics:[(&#39;average_q&#39;, 9.944943), (&#39;average_loss&#39;, 0.06969788812333717), (&#39;cumulative_steps&#39;, 30350), (&#39;n_updates&#39;, 29851), (&#39;rlen&#39;, 30350)]
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>outdir:result step:764 episode:3 R:164.0
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>statistics:[(&#39;average_q&#39;, 10.058809), (&#39;average_loss&#39;, 0.05239647082518786), (&#39;cumulative_steps&#39;, 30514), (&#39;n_updates&#39;, 30015), (&#39;rlen&#39;, 30514)]
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>outdir:result step:957 episode:4 R:193.0
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>statistics:[(&#39;average_q&#39;, 9.991211), (&#39;average_loss&#39;, 0.07847402240789961), (&#39;cumulative_steps&#39;, 30707), (&#39;n_updates&#39;, 30208), (&#39;rlen&#39;, 30707)]
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>outdir:result step:1157 episode:5 R:200.0
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>statistics:[(&#39;average_q&#39;, 10.015636), (&#39;average_loss&#39;, 0.0489872798131546), (&#39;cumulative_steps&#39;, 30907), (&#39;n_updates&#39;, 30408), (&#39;rlen&#39;, 30907)]
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>evaluation episode 0 length:165 R:165.0
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>evaluation episode 1 length:165 R:165.0
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>evaluation episode 2 length:149 R:149.0
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>evaluation episode 3 length:144 R:144.0
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>evaluation episode 4 length:146 R:146.0
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>evaluation episode 5 length:147 R:147.0
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>evaluation episode 6 length:143 R:143.0
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>evaluation episode 7 length:172 R:172.0
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>evaluation episode 8 length:189 R:189.0
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>evaluation episode 9 length:144 R:144.0
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>The best score is updated -3.4028235e+38 -&gt; 156.4
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Saved the agent to result/best
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>outdir:result step:1279 episode:6 R:122.0
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>statistics:[(&#39;average_q&#39;, 10.050219), (&#39;average_loss&#39;, 0.05810094685410149), (&#39;cumulative_steps&#39;, 31029), (&#39;n_updates&#39;, 30530), (&#39;rlen&#39;, 31029)]
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>outdir:result step:1461 episode:7 R:182.0
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>statistics:[(&#39;average_q&#39;, 10.083727), (&#39;average_loss&#39;, 0.044854263817542234), (&#39;cumulative_steps&#39;, 31211), (&#39;n_updates&#39;, 30712), (&#39;rlen&#39;, 31211)]
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>outdir:result step:1639 episode:8 R:178.0
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>statistics:[(&#39;average_q&#39;, 10.096574), (&#39;average_loss&#39;, 0.06585528045892715), (&#39;cumulative_steps&#39;, 31389), (&#39;n_updates&#39;, 30890), (&#39;rlen&#39;, 31389)]
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>outdir:result step:1839 episode:9 R:200.0
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>statistics:[(&#39;average_q&#39;, 10.077509), (&#39;average_loss&#39;, 0.053921366051072256), (&#39;cumulative_steps&#39;, 31589), (&#39;n_updates&#39;, 31090), (&#39;rlen&#39;, 31589)]
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>outdir:result step:2000 episode:10 R:161.0
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>statistics:[(&#39;average_q&#39;, 10.080789), (&#39;average_loss&#39;, 0.067765186370234), (&#39;cumulative_steps&#39;, 31750), (&#39;n_updates&#39;, 31251), (&#39;rlen&#39;, 31750)]
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>evaluation episode 0 length:200 R:200.0
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>evaluation episode 1 length:200 R:200.0
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>evaluation episode 2 length:200 R:200.0
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>evaluation episode 3 length:200 R:200.0
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>evaluation episode 4 length:200 R:200.0
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>evaluation episode 5 length:200 R:200.0
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>evaluation episode 6 length:200 R:200.0
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>evaluation episode 7 length:200 R:200.0
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>evaluation episode 8 length:200 R:200.0
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>evaluation episode 9 length:200 R:200.0
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>The best score is updated 156.4 -&gt; 200.0
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Saved the agent to result/best
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Saved the agent to result/2000_finish
</pre></div>
</div>
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(&lt;pfrl.agents.double_dqn.DoubleDQN at 0x107c994c0&gt;,
 [{&#39;average_q&#39;: 10.015636,
   &#39;average_loss&#39;: 0.0489872798131546,
   &#39;cumulative_steps&#39;: 30907,
   &#39;n_updates&#39;: 30408,
   &#39;rlen&#39;: 30907,
   &#39;eval_score&#39;: 156.4},
  {&#39;average_q&#39;: 10.080789,
   &#39;average_loss&#39;: 0.067765186370234,
   &#39;cumulative_steps&#39;: 31750,
   &#39;n_updates&#39;: 31251,
   &#39;rlen&#39;: 31750,
   &#39;eval_score&#39;: 200.0}])
</pre></div>
</div>
</div>
</div>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="rainbow-dqn">
<h1>Rainbow DQN<a class="headerlink" href="#rainbow-dqn" title="Permalink to this headline">#</a></h1>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">pfrl.q_functions</span> <span class="kn">import</span> <span class="n">DistributionalDuelingDQN</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">n_atoms</span> <span class="o">=</span> <span class="mi">51</span>
<span class="n">v_max</span> <span class="o">=</span> <span class="mi">10</span>
<span class="n">v_min</span> <span class="o">=</span> <span class="o">-</span><span class="mi">10</span>

<span class="n">q_func</span> <span class="o">=</span> <span class="n">q_functions</span><span class="o">.</span><span class="n">DistributionalFCStateQFunctionWithDiscreteAction</span><span class="p">(</span>
        <span class="n">obs_size</span><span class="p">,</span>
        <span class="n">n_actions</span><span class="p">,</span>
        <span class="n">n_atoms</span><span class="p">,</span>
        <span class="n">v_min</span><span class="p">,</span>
        <span class="n">v_max</span><span class="p">,</span>
        <span class="n">n_hidden_channels</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">n_hidden_channels</span><span class="p">,</span>
        <span class="n">n_hidden_layers</span><span class="o">=</span><span class="n">args</span><span class="o">.</span><span class="n">n_hidden_layers</span><span class="p">,</span>
<span class="p">)</span><span class="nb">print</span><span class="p">(</span><span class="n">q_func</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output traceback highlight-ipythontb notranslate"><div class="highlight"><pre><span></span>  <span class="n">Input</span> <span class="n">In</span> <span class="p">[</span><span class="mi">11</span><span class="p">]</span>
    <span class="p">)</span><span class="nb">print</span><span class="p">(</span><span class="n">q_func</span><span class="p">)</span>
     <span class="o">^</span>
<span class="ne">SyntaxError</span>: invalid syntax
</pre></div>
</div>
</div>
</div>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./reinforcement"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
            </main>
            <footer class="footer-article noprint">
                
    <!-- Previous / next buttons -->
<div class='prev-next-area'>
</div>
            </footer>
        </div>
    </div>
    <div class="footer-content row">
        <footer class="col footer"><p>
  
    By Puneet Girdhar<br/>
  
      &copy; Copyright 2022.<br/>
</p>
        </footer>
    </div>
    
</div>


      </div>
    </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf"></script>


  </body>
</html>