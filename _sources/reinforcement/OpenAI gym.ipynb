{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bc6a7270",
   "metadata": {},
   "source": [
    "# OpenAI gym\n",
    "\n",
    "OpenAI is an **artificial intlligence (AI)** research organization that aims to build **artificial general intelligence (AGI)**. OpenAI provides a fmaous toolkit called Gym for training a reinforcement learning agent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0dccce3",
   "metadata": {},
   "source": [
    "## Creating our first Gym environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f3895af",
   "metadata": {},
   "source": [
    "Let's introduce one of the simplest environments called Frozen Lake environment. As we can observe, in the frozen environment, the goal of the agent is to start from the initial state S and reach the goal state **G**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08405bb1",
   "metadata": {},
   "source": [
    "- S denotes the starting state\n",
    "- F denotes the frozen state\n",
    "- H denotes the hole state\n",
    "- G denotes the goal state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2285d2f2",
   "metadata": {},
   "source": [
    "### Goal\n",
    "\n",
    "Goal of the agent is to start from state **S** and reach the goal state **G** without touching **H**. It can only travel over F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fc73f83",
   "metadata": {},
   "source": [
    "## Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "287c79a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "94aa2607",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = gym.make('FrozenLake-v1')\n",
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7263dfcb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dd422b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "887f8d43",
   "metadata": {},
   "source": [
    "## Exploring the environment\n",
    "\n",
    "In the previous chapter, we learned that the reinforcement learning environment can be modeled as **Markov Decision Process (MDP)** and an MDP consists of the following:\n",
    "\n",
    "- **States**\n",
    "- **Actions**\n",
    "- **Transitino probability**\n",
    "- **Reward function**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e1f6a7c",
   "metadata": {},
   "source": [
    "### States\n",
    "\n",
    "A state space consists of all of our statgse. We can obtain the number of states in our environment as below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e4019a12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discrete(16)\n"
     ]
    }
   ],
   "source": [
    "print(env.observation_space)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fba567ab",
   "metadata": {},
   "source": [
    "It implies that we have 16 discrete states in our state space starting from states **S** to **G**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b289c1c0",
   "metadata": {},
   "source": [
    "### Actions\n",
    "\n",
    "We learned that action space consts of all possible actions. In our case we have 4 discrete actions in our action space, which are **left, down, right and up**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0767cb30",
   "metadata": {},
   "source": [
    "### Transition probability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7de188e8",
   "metadata": {},
   "source": [
    "It is a stochastic environment, we cannot say that by performing some action **a**, the agent will always reach the enxt state **s** We also reach other states with some probability. So when we perform an action 1 (down) in state 2, we reach state 1 with probability (0.33), we reach state 6 with probability 0.333 and we reach state 3 with the same probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6166a521",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.3333333333333333, 2, 0.0, False),\n",
       " (0.3333333333333333, 1, 0.0, False),\n",
       " (0.3333333333333333, 6, 0.0, False)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.P[2][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24e4a6ad",
   "metadata": {},
   "source": [
    "Our output is in the form of (transition probability, next state, reward, is Terminal )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "297c3daf",
   "metadata": {},
   "source": [
    "## Generating an episode"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed704052",
   "metadata": {},
   "source": [
    "In order for an agent to interact with the environment, it has to perform some action in the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8469e66d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 0.0, False, {'prob': 0.3333333333333333})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.step(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "92785807",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d418546",
   "metadata": {},
   "source": [
    "Episode is the agent environment interaction startin from initial state to terminal state. An episode ends if the agent reaches the terminal state. So, in the frozen lake environment, the episode will end if agent reaches the terminal state.\n",
    "\n",
    "Let's understand how to generate an episode with the random policy. We learned that the random policy selects a random action in each state. So we will generate an episode by taking random actions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f361b8e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_episodes = 10\n",
    "num_timesteps = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6e1de642",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode #  0\n",
      "time step 1\n",
      "time step 2\n",
      "time step 3\n",
      "time step 4\n",
      "time step 5\n",
      "Episode #  1\n",
      "time step 1\n",
      "time step 2\n",
      "time step 3\n",
      "time step 4\n",
      "time step 5\n",
      "time step 6\n",
      "time step 7\n",
      "time step 8\n",
      "Episode #  2\n",
      "time step 1\n",
      "time step 2\n",
      "time step 3\n",
      "time step 4\n",
      "time step 5\n",
      "time step 6\n",
      "time step 7\n",
      "time step 8\n",
      "time step 9\n",
      "time step 10\n",
      "time step 11\n",
      "time step 12\n",
      "time step 13\n",
      "time step 14\n",
      "time step 15\n",
      "time step 16\n",
      "time step 17\n",
      "time step 18\n",
      "time step 19\n",
      "time step 20\n",
      "Episode #  3\n",
      "time step 1\n",
      "time step 2\n",
      "Episode #  4\n",
      "time step 1\n",
      "time step 2\n",
      "Episode #  5\n",
      "time step 1\n",
      "time step 2\n",
      "time step 3\n",
      "time step 4\n",
      "time step 5\n",
      "time step 6\n",
      "time step 7\n",
      "time step 8\n",
      "time step 9\n",
      "time step 10\n",
      "time step 11\n",
      "time step 12\n",
      "time step 13\n",
      "time step 14\n",
      "time step 15\n",
      "time step 16\n",
      "time step 17\n",
      "time step 18\n",
      "time step 19\n",
      "time step 20\n",
      "Episode #  6\n",
      "time step 1\n",
      "time step 2\n",
      "time step 3\n",
      "time step 4\n",
      "time step 5\n",
      "time step 6\n",
      "time step 7\n",
      "time step 8\n",
      "time step 9\n",
      "time step 10\n",
      "time step 11\n",
      "time step 12\n",
      "time step 13\n",
      "time step 14\n",
      "time step 15\n",
      "time step 16\n",
      "Episode #  7\n",
      "time step 1\n",
      "time step 2\n",
      "time step 3\n",
      "time step 4\n",
      "time step 5\n",
      "time step 6\n",
      "time step 7\n",
      "time step 8\n",
      "Episode #  8\n",
      "time step 1\n",
      "time step 2\n",
      "time step 3\n",
      "time step 4\n",
      "time step 5\n",
      "time step 6\n",
      "Episode #  9\n",
      "time step 1\n",
      "time step 2\n",
      "time step 3\n",
      "time step 4\n"
     ]
    }
   ],
   "source": [
    "for i in range(num_episodes):\n",
    "    print('Episode # ', i)\n",
    "    state = env.reset()\n",
    "    for t in range(num_timesteps):\n",
    "        \n",
    "        random_action = env.action_space.sample()\n",
    "        next_state, reward, done, info = env.step(random_action)\n",
    "        print('time step', t+1)\n",
    "        env.render()\n",
    "        if done:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f5fa48d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
