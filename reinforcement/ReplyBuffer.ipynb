{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a3171d0c",
   "metadata": {},
   "source": [
    "# Replay Buffer\n",
    "\n",
    "Replay Buffer to store trajectories of experience when executing a policy in an environment. During training, replay buffers are queried for a subset of the trajectories ( either a sequential subset or a sample ) to \"replay\" the agent's exeprience"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb9d71f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "from tf_agents import specs\n",
    "from tf_agents.agents.dqn import dqn_agent\n",
    "from tf_agents.drivers import dynamic_step_driver\n",
    "from tf_agents.environments import suite_gym\n",
    "from tf_agents.environments import tf_py_environment\n",
    "from tf_agents.networks import q_network\n",
    "from tf_agents.replay_buffers import py_uniform_replay_buffer\n",
    "from tf_agents.replay_buffers import tf_uniform_replay_buffer\n",
    "from tf_agents.specs import tensor_spec\n",
    "from tf_agents.trajectories import time_step, Trajectory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e82fd747",
   "metadata": {},
   "source": [
    "# Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "342928b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "env_name = \"CartPole-v0\" # @param {type:\"string\"}\n",
    "num_iterations = 250 # @param {type:\"integer\"}\n",
    "collect_episodes_per_iteration = 2 # @param {type:\"integer\"}\n",
    "replay_buffer_capacity = 2000 # @param {type:\"integer\"}\n",
    "\n",
    "fc_layer_params = (100,)\n",
    "\n",
    "learning_rate = 1e-3 # @param {type:\"number\"}\n",
    "log_interval = 25 # @param {type:\"integer\"}\n",
    "num_eval_episodes = 10 # @param {type:\"integer\"}\n",
    "eval_interval = 50 # @param {type:\"integer\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9718919",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_env = suite_gym.load(env_name)\n",
    "eval_env = suite_gym.load(env_name)\n",
    "\n",
    "train_tf_env = tf_py_environment.TFPyEnvironment(train_env)\n",
    "eval_tf_env = tf_py_environment.TFPyEnvironment(eval_env)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31923d07",
   "metadata": {},
   "source": [
    "It has two elements, `time_step_spec` and `action_spec`. These two will go to trajectory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e027e31c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('action_spec:', train_env.action_spec())\n",
    "print(train_env.time_step_spec())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10c9bf15",
   "metadata": {},
   "source": [
    "We see that environment expects action of type `int64` and rturns `TimeSteps` where observation are a `float32` vector of length 4 and discount factor is `float32`. now let's try to take a fixed action `(1,)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aea1faf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "action = np.array(1, dtype=np.int32)\n",
    "time_step = train_env.reset()\n",
    "print(time_step)\n",
    "while not time_step.is_last():\n",
    "    time_step = train_env.step(action)\n",
    "    print(time_step)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2511e36e",
   "metadata": {},
   "source": [
    "# Store the transitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b67480b",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_step = train_env.reset()\n",
    "rewards = []\n",
    "steps = []\n",
    "num_episodes = 5\n",
    "for _ in range(num_episodes):\n",
    "    episode_reward = 0\n",
    "    episode_steps = 0\n",
    "    while not time_step.is_last():\n",
    "        action = np.random.choice([0,1])\n",
    "        time_step = train_env.step(action)\n",
    "        episode_steps += 1\n",
    "        episode_reward += time_step.reward\n",
    "        \n",
    "    rewards.append(episode_reward)\n",
    "    steps.append(episode_steps)\n",
    "    time_step = train_env.reset()\n",
    "    \n",
    "num_steps = np.sum(steps)\n",
    "avg_length = np.mean(steps)\n",
    "avg_reward = np.mean(rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5297084d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('num_episodes:', num_episodes, 'num_steps:', num_steps)\n",
    "print('avg_length', avg_length, 'avg_reward:', avg_reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d3ab060",
   "metadata": {},
   "source": [
    "# Replay Buffers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41027632",
   "metadata": {},
   "source": [
    "`PyUniformReplayBuffers` can be used to store the episodes and convert to batch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da55d6c0",
   "metadata": {},
   "source": [
    "For most agents, `collect_data_spec` is a named tuple called `Trajectory`, containing the specs for observations, actions, rewards, and other items.\"\"\"\n",
    "\n",
    "agent.collect_data_spec\n",
    "\n",
    "agent.collect_data_spec._fields\n",
    "\n",
    "## Data Collection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1954263",
   "metadata": {},
   "source": [
    "The most important method is action(time_step) which maps a time_step containing an observation from the environment to a PolicyStep named tuple containing the following attributes:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3143cd88",
   "metadata": {},
   "source": [
    "* action: The action to be applied to the environment.\n",
    "* state: The state of the policy (e.g. RNN state) to be fed into the next call to action.\n",
    "* info: Optional side information such as action log probabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac7b92f2",
   "metadata": {},
   "source": [
    "Environment provides two spec \n",
    "* env.time_step_spec() \n",
    "* env.action_spec()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c73477e",
   "metadata": {},
   "outputs": [],
   "source": [
    "replay_buffer_capacity = 1000*32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abbae939",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tf_agents.agents import data_converter\n",
    "from tf_agents.specs import tensor_spec\n",
    "from tf_agents.trajectories import time_step as ts\n",
    "from tf_agents.trajectories import trajectory\n",
    "from tf_agents.utils import test_utils\n",
    "from tf_agents.trajectories import policy_step\n",
    "from tf_agents.specs import array_spec\n",
    "from tf_agents.utils import nest_utils\n",
    "\n",
    "from tf_agents.drivers import py_driver\n",
    "from tf_agents.policies import py_tf_eager_policy\n",
    "from tf_agents.replay_buffers import episodic_replay_buffer\n",
    "\n",
    "from tf_agents.drivers import dynamic_episode_driver\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfdf831c",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_step_spec = train_env.time_step_spec()\n",
    "action_spec = policy_step.PolicyStep(train_env.action_spec())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "166cb990",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_spec = trajectory.from_transition(time_step_spec, action_spec, time_step_spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "938e17b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "py_replay_buffer = episodic_replay_buffer.EpisodicReplayBuffer(\n",
    "    capacity=2,\n",
    "    data_spec=data_spec,\n",
    "    completed_only=True)\n",
    "\n",
    "stateful_buffer = episodic_replay_buffer.StatefulEpisodicReplayBuffer(\n",
    "    py_replay_buffer,\n",
    "    num_episodes=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "950cef4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_step(environment, policy, buffer, id, policy_state):\n",
    "  time_step = environment.current_time_step()\n",
    "  if policy_state:\n",
    "      action_step = policy.action(time_step, policy_state)\n",
    "      policy_state = action_step.state\n",
    "  else:\n",
    "      action_step = policy.action(time_step)\n",
    "\n",
    "  next_time_step = environment.step(action_step.action)\n",
    "  traj = trajectory.from_transition(time_step, action_step, next_time_step)\n",
    "\n",
    "  id_tensor = tf.constant(id, dtype=tf.int64)\n",
    "  buffer.add_batch(traj, id_tensor)\n",
    "  if time_step.is_last():\n",
    "      id[0] += 1\n",
    "\n",
    "  return policy_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3e0971d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_data(env, policy, buffer, steps, id, policy_state=()):\n",
    "  for _ in range(steps):\n",
    "    policy_state = collect_step(env, policy, buffer, id, policy_state)\n",
    "\n",
    "  return policy_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6360af3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "episode_id = [0]\n",
    "collect_data(train_env, random_policy, py_replay_buffer, 2 , episode_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "076e6ef5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4326af1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def collect_episode(environment, policy, num_episodes):\n",
    "\n",
    "  driver = py_driver.PyDriver(\n",
    "    environment,\n",
    "    py_tf_eager_policy.PyTFEagerPolicy(\n",
    "      policy, use_tf_function=True),\n",
    "    [py_replay_buffer.add_batch],\n",
    "    max_episodes=num_episodes)\n",
    "  initial_time_step = environment.reset()\n",
    "  driver.run(initial_time_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a39013f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "collect_episode(train_env, tf_agent.collect_policy, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2577db6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_episode(environment, policy, buffer=None, steps=2):\n",
    "    observers = [buffer.add_batch]\n",
    "    \n",
    "    driver = dynamic_episode_driver.DynamicEpisodeDriver(\n",
    "            environment, policy, observers,num_episodes=steps)\n",
    "    \n",
    "    final_time_step, policy_state = driver.run()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89a1740b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@test {\"skip\": true}\n",
    "def collect_step(environment, policy, buffer):\n",
    "  time_step = environment.current_time_step()\n",
    "  action_step = policy.action(time_step)\n",
    "  next_time_step = environment.step(action_step.action)\n",
    "  traj = trajectory.from_transition(time_step, action_step, next_time_step)\n",
    "\n",
    "  #print(traj)\n",
    "  # Add trajectory to the replay buffer\n",
    "  buffer.add_batch(nest_utils.batch_nested_array(traj))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce3b20f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tf_agents.policies import random_py_policy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27f3ff7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_policy = random_py_policy.RandomPyPolicy(train_env.time_step_spec(),\n",
    "                                                train_env.action_spec())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fee17a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_data(env, policy, buffer, steps):\n",
    "  for _ in range(steps):\n",
    "    collect_step(env, policy, buffer)\n",
    "\n",
    "collect_data(train_env, random_policy, py_replay_buffer, steps=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e67e4a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "collect_episode(train_tf_env, random_policy, py_replay_buffer, steps=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "827fb0f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = py_replay_buffer.as_dataset(\n",
    "    sample_batch_size=batch_size,\n",
    "    num_steps=n_step_update + 1).prefetch(3)\n",
    "\n",
    "iterator = iter(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55a7a892",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84b53fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_avg_return(environment, policy, num_episodes = 10):\n",
    "    total_return = 0.0\n",
    "    for _ in range(num_episodes):\n",
    "        time_step = environment.reset()\n",
    "        episode_return = 0.0\n",
    "        \n",
    "        while not time_step.is_last():\n",
    "            action_step = policy.action(time_step)\n",
    "            time_step = environment.step(action_step.action)\n",
    "            episode_return += time_step.reward\n",
    "            \n",
    "        total_return += episode_return\n",
    "        \n",
    "    avg_return = total_return / num_episodes\n",
    "    return avg_return.numpy()[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc74d587",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tf_env = tf_py_environment.TFPyEnvironment(train_env)\n",
    "eval_tf_env = tf_py_environment.TFPyEnvironment(eval_env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f8d30d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tf_agents.networks import actor_distribution_network\n",
    "from tf_agents.policies import py_tf_eager_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a5cf328",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fc_layer_params = (100,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f337269",
   "metadata": {},
   "outputs": [],
   "source": [
    "actor_net = actor_distribution_network.ActorDistributionNetwork(\n",
    "    train_tf_env.observation_spec(),\n",
    "    train_tf_env.action_spec(),\n",
    "    fc_layer_params=fc_layer_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d6ccd95",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tf_agents.agents.reinforce import reinforce_agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf0973f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "\n",
    "train_step_counter = tf.Variable(0)\n",
    "\n",
    "tf_agent = reinforce_agent.ReinforceAgent(\n",
    "    train_env.time_step_spec(),\n",
    "    train_env.action_spec(),\n",
    "    actor_network=actor_net,\n",
    "    optimizer=optimizer,\n",
    "    normalize_returns=True,\n",
    "    train_step_counter=train_step_counter)\n",
    "tf_agent.initialize()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8714b705",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_agent.train_step_counter.assign(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05853fde",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_return = compute_avg_return(eval_tf_env, tf_agent.policy, num_eval_episodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f73ece8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "returns = [avg_return]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92372114",
   "metadata": {},
   "outputs": [],
   "source": [
    "returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c2da24b",
   "metadata": {},
   "outputs": [],
   "source": [
    "collect_episode(train_tf_env, tf_agent.collect_policy, py_replay_buffer, steps=2)\n",
    "\n",
    "    \n",
    "iterator = iter(py_replay_buffer.as_dataset(sample_batch_size=1))\n",
    "trajectories  = next(iterator)\n",
    "print(trajectories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63d4e9b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "py_replay_buffer.clear()\n",
    "\n",
    "for _ in range(num_iterations):\n",
    "    # collect few episodes using collect_policy and save to replay buffer\n",
    "    collect_episode(train_tf_env, tf_agent.collect_policy, py_replay_buffer, steps=2)\n",
    "    \n",
    "    iterator = iter(py_replay_buffer.as_dataset(sample_batch_size=1))\n",
    "    trajectories  = next(iterator)\n",
    "    print(trajectories)\n",
    "    batched_exp = tf.nest.map_structure(\n",
    "        lambda t: tf.expand_dims(t, axis=0),\n",
    "        trajectories\n",
    "    )\n",
    "    \n",
    "    train_loss = tf_agent.train(batched_exp).loss\n",
    "    \n",
    "    py_replay_buffer.clear()\n",
    "    \n",
    "    step = tf_agent.train_step_counter.numpy()\n",
    "    \n",
    "    if step % log_interval == 0:\n",
    "        print('step == {0}: loss = {1}'.format(step, train_loss.loss))\n",
    "        \n",
    "    if step % eval_interval == 0:\n",
    "        avg_return = compute_avg_return(eval_tf_env, tf_agent.policy, num_eval_episodes)\n",
    "        print('step = {0}: Average return = {1}'.format(step, avg_return))\n",
    "        \n",
    "        returns.append(avg_return)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c73e1e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = range(0, num_iterations + 1, eval_interval)\n",
    "plt.plot(steps, returns)\n",
    "plt.ylabel('Average Return')\n",
    "plt.xlabel('Step')\n",
    "plt.ylim(top=250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eb9b0d8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
