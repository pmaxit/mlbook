{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "14bcb552",
   "metadata": {},
   "source": [
    "# Huggingface datasets\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "949c4f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Most basic stuff for EDA.\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', 50)\n",
    "pd.set_option('display.max_rows', 150)\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "\n",
    "# !pip install transformers\n",
    "# !pip install datasets\n",
    "from datasets import load_dataset, load_metric\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer \n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a737b0a",
   "metadata": {},
   "source": [
    "# Read the data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fcd366d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tweets = pd.read_csv('../data/train.csv')\n",
    "test_tweets = pd.read_csv('../data/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2f0f950c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
       "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
       "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
       "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
       "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
       "\n",
       "   target  \n",
       "0       1  \n",
       "1       1  \n",
       "2       1  \n",
       "3       1  \n",
       "4       1  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_tweets.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e945beb3",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3af15c36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='count', ylabel='target'>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.set_style('whitegrid')\n",
    "sns.countplot(y=train_tweets['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8884b362",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "USA                104\n",
       "New York            71\n",
       "United States       50\n",
       "London              45\n",
       "Canada              29\n",
       "Nigeria             28\n",
       "UK                  27\n",
       "Los Angeles, CA     26\n",
       "India               24\n",
       "Mumbai              22\n",
       "Washington, DC      21\n",
       "Kenya               20\n",
       "Worldwide           19\n",
       "Australia           18\n",
       "Chicago, IL         18\n",
       "California          17\n",
       "Everywhere          15\n",
       "New York, NY        15\n",
       "California, USA     15\n",
       "Florida             14\n",
       "Name: location, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_tweets['location'].value_counts().head(n=20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f412867e",
   "metadata": {},
   "source": [
    "Let's clean the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "005204fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some basic helper functions to clean text by removing urls, emojis, html tags and punctuations.\n",
    "import re\n",
    "import string\n",
    "\n",
    "def remove_URL(text):\n",
    "    url = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    return url.sub(r'', text)\n",
    "\n",
    "\n",
    "def remove_emoji(text):\n",
    "    emoji_pattern = re.compile(\n",
    "        '['\n",
    "        u'\\U0001F600-\\U0001F64F'  # emoticons\n",
    "        u'\\U0001F300-\\U0001F5FF'  # symbols & pictographs\n",
    "        u'\\U0001F680-\\U0001F6FF'  # transport & map symbols\n",
    "        u'\\U0001F1E0-\\U0001F1FF'  # flags (iOS)\n",
    "        u'\\U00002702-\\U000027B0'\n",
    "        u'\\U000024C2-\\U0001F251'\n",
    "        ']+',\n",
    "        flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', text)\n",
    "\n",
    "\n",
    "def remove_html(text):\n",
    "    html = re.compile(r'<.*?>|&([a-z0-9]+|#[0-9]{1,6}|#x[0-9a-f]{1,6});')\n",
    "    return re.sub(html, '', text)\n",
    "\n",
    "\n",
    "def remove_punct(text):\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    return text.translate(table)\n",
    "\n",
    "# Applying helper functions on Train Dataset\n",
    "\n",
    "train_tweets['text_clean'] = train_tweets['text'].apply(lambda x: remove_URL(x))\n",
    "train_tweets['text_clean'] = train_tweets['text_clean'].apply(lambda x: remove_emoji(x))\n",
    "train_tweets['text_clean'] = train_tweets['text_clean'].apply(lambda x: remove_html(x))\n",
    "train_tweets['text_clean'] = train_tweets['text_clean'].apply(lambda x: remove_punct(x))\n",
    "\n",
    "# Applying helper functions on Test Dataset\n",
    "\n",
    "test_tweets['text_clean'] = test_tweets['text'].apply(lambda x: remove_URL(x))\n",
    "test_tweets['text_clean'] = test_tweets['text_clean'].apply(lambda x: remove_emoji(x))\n",
    "test_tweets['text_clean'] = test_tweets['text_clean'].apply(lambda x: remove_html(x))\n",
    "test_tweets['text_clean'] = test_tweets['text_clean'].apply(lambda x: remove_punct(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "acf23a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tweets.fillna(\"\",inplace=True)\n",
    "test_tweets.fillna(\"\",inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f7a5d0f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_nm = 'microsoft/deberta-v3-small'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8c0a63e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "tokz = AutoTokenizer.from_pretrained(model_nm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "594c7102",
   "metadata": {},
   "outputs": [],
   "source": [
    "sep = tokz.sep_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "33bc7b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tweets['train'] = train_tweets['text_clean'] + sep + train_tweets['location'] + sep + train_tweets['keyword']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "81032e2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>text_clean</th>\n",
       "      <th>train</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "      <td>Our Deeds are the Reason of this earthquake Ma...</td>\n",
       "      <td>Our Deeds are the Reason of this earthquake Ma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "      <td>Forest fire near La Ronge Sask Canada</td>\n",
       "      <td>Forest fire near La Ronge Sask Canada[SEP][SEP]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "      <td>All residents asked to shelter in place are be...</td>\n",
       "      <td>All residents asked to shelter in place are be...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "      <td>13000 people receive wildfires evacuation orde...</td>\n",
       "      <td>13000 people receive wildfires evacuation orde...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "      <td>Just got sent this photo from Ruby Alaska as s...</td>\n",
       "      <td>Just got sent this photo from Ruby Alaska as s...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1                   Our Deeds are the Reason of this #earthquake M...   \n",
       "1   4                              Forest fire near La Ronge Sask. Canada   \n",
       "2   5                   All residents asked to 'shelter in place' are ...   \n",
       "3   6                   13,000 people receive #wildfires evacuation or...   \n",
       "4   7                   Just got sent this photo from Ruby #Alaska as ...   \n",
       "\n",
       "   target                                         text_clean  \\\n",
       "0       1  Our Deeds are the Reason of this earthquake Ma...   \n",
       "1       1              Forest fire near La Ronge Sask Canada   \n",
       "2       1  All residents asked to shelter in place are be...   \n",
       "3       1  13000 people receive wildfires evacuation orde...   \n",
       "4       1  Just got sent this photo from Ruby Alaska as s...   \n",
       "\n",
       "                                               train  \n",
       "0  Our Deeds are the Reason of this earthquake Ma...  \n",
       "1    Forest fire near La Ronge Sask Canada[SEP][SEP]  \n",
       "2  All residents asked to shelter in place are be...  \n",
       "3  13000 people receive wildfires evacuation orde...  \n",
       "4  Just got sent this photo from Ruby Alaska as s...  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_tweets.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5f05d26",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4385eaf2",
   "metadata": {},
   "source": [
    "Time to import some stuff we'll need for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6b9a45bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import warnings,transformers,logging,torch\n",
    "from transformers import TrainingArguments,Trainer\n",
    "from transformers import AutoModelForSequenceClassification,AutoTokenizer\n",
    "\n",
    "from datasets import load_dataset, Dataset, DatasetDict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6a6b7379",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tok_func(x):\n",
    "    return tokz(x['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "53d81266",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = Dataset.from_pandas(train_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e1606208",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'keyword', 'location', 'text', 'target', 'text_clean', 'train'], dtype='object')"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_tweets.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "44fa3e0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8912f8eb8bb748b196e4417ca4b61dc4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tok_ds = ds.map(tok_func, batched=True, remove_columns=['id', 'keyword', 'location', 'text', 'target', 'text_clean'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "fdf73815",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': 'Our Deeds are the Reason of this earthquake May ALLAH Forgive us all[SEP][SEP]',\n",
       " 'input_ids': [1,\n",
       "  581,\n",
       "  65453,\n",
       "  281,\n",
       "  262,\n",
       "  18037,\n",
       "  265,\n",
       "  291,\n",
       "  10612,\n",
       "  903,\n",
       "  4924,\n",
       "  17018,\n",
       "  43632,\n",
       "  381,\n",
       "  305,\n",
       "  2,\n",
       "  2,\n",
       "  2],\n",
       " 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       " 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tok_ds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "013dc88e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dds = tok_ds.train_test_split(test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21b48780",
   "metadata": {},
   "source": [
    "# Initial model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "86b96ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr,bs = 8e-5,128\n",
    "wd,epochs = 0.01,4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "816dc04c",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'torch.distributed' has no attribute 'is_initialized'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [50]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m args \u001b[38;5;241m=\u001b[39m \u001b[43mTrainingArguments\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43moutputs\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwarmup_ratio\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr_scheduler_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcosine\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfp16\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mevaluation_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mepoch\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mper_device_train_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mper_device_eval_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbs\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_train_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreport_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mnone\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m<string>:93\u001b[0m, in \u001b[0;36m__init__\u001b[0;34m(self, output_dir, overwrite_output_dir, do_train, do_eval, do_predict, evaluation_strategy, prediction_loss_only, per_device_train_batch_size, per_device_eval_batch_size, per_gpu_train_batch_size, per_gpu_eval_batch_size, gradient_accumulation_steps, eval_accumulation_steps, eval_delay, learning_rate, weight_decay, adam_beta1, adam_beta2, adam_epsilon, max_grad_norm, num_train_epochs, max_steps, lr_scheduler_type, warmup_ratio, warmup_steps, log_level, log_level_replica, log_on_each_node, logging_dir, logging_strategy, logging_first_step, logging_steps, logging_nan_inf_filter, save_strategy, save_steps, save_total_limit, save_on_each_node, no_cuda, seed, data_seed, bf16, fp16, fp16_opt_level, half_precision_backend, bf16_full_eval, fp16_full_eval, tf32, local_rank, xpu_backend, tpu_num_cores, tpu_metrics_debug, debug, dataloader_drop_last, eval_steps, dataloader_num_workers, past_index, run_name, disable_tqdm, remove_unused_columns, label_names, load_best_model_at_end, metric_for_best_model, greater_is_better, ignore_data_skip, sharded_ddp, deepspeed, label_smoothing_factor, optim, adafactor, group_by_length, length_column_name, report_to, ddp_find_unused_parameters, ddp_bucket_cap_mb, dataloader_pin_memory, skip_memory_metrics, use_legacy_prediction_loop, push_to_hub, resume_from_checkpoint, hub_model_id, hub_strategy, hub_token, hub_private_repo, gradient_checkpointing, include_inputs_for_metrics, fp16_backend, push_to_hub_model_id, push_to_hub_organization, push_to_hub_token, mp_parameters)\u001b[0m\n",
      "File \u001b[0;32m~/miniforge3/envs/meta/lib/python3.9/site-packages/transformers/training_args.py:876\u001b[0m, in \u001b[0;36mTrainingArguments.__post_init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    868\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    869\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`--adafactor` is deprecated and will be removed in version 5 of 🤗 Transformers. Use `--optim adafactor` instead\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    870\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[1;32m    871\u001b[0m     )\n\u001b[1;32m    872\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptim \u001b[38;5;241m=\u001b[39m OptimizerNames\u001b[38;5;241m.\u001b[39mADAFACTOR\n\u001b[1;32m    874\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    875\u001b[0m     is_torch_available()\n\u001b[0;32m--> 876\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[38;5;241m.\u001b[39mtype \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    877\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice\u001b[38;5;241m.\u001b[39mtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGPU_NUM_DEVICES\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39menviron)\n\u001b[1;32m    878\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfp16 \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfp16_full_eval \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbf16 \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbf16_full_eval)\n\u001b[1;32m    879\u001b[0m ):\n\u001b[1;32m    880\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    881\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMixed precision training with AMP or APEX (`--fp16` or `--bf16`) and half precision evaluation (`--fp16_full_eval` or `--bf16_full_eval`) can only be used on CUDA devices.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    882\u001b[0m     )\n\u001b[1;32m    884\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_torch_available() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtf32 \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniforge3/envs/meta/lib/python3.9/site-packages/transformers/utils/import_utils.py:781\u001b[0m, in \u001b[0;36mtorch_required.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    778\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[1;32m    779\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    780\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_torch_available():\n\u001b[0;32m--> 781\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    782\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    783\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMethod `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m` requires PyTorch.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniforge3/envs/meta/lib/python3.9/site-packages/transformers/training_args.py:1110\u001b[0m, in \u001b[0;36mTrainingArguments.device\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1104\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[1;32m   1105\u001b[0m \u001b[38;5;129m@torch_required\u001b[39m\n\u001b[1;32m   1106\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdevice\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch.device\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   1107\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[38;5;124;03m    The device used by this process.\u001b[39;00m\n\u001b[1;32m   1109\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_setup_devices\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/meta/lib/python3.9/site-packages/transformers/utils/generic.py:48\u001b[0m, in \u001b[0;36mcached_property.__get__\u001b[0;34m(self, obj, objtype)\u001b[0m\n\u001b[1;32m     46\u001b[0m cached \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(obj, attr, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cached \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 48\u001b[0m     cached \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;28msetattr\u001b[39m(obj, attr, cached)\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m cached\n",
      "File \u001b[0;32m~/miniforge3/envs/meta/lib/python3.9/site-packages/transformers/utils/import_utils.py:781\u001b[0m, in \u001b[0;36mtorch_required.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    778\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[1;32m    779\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    780\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_torch_available():\n\u001b[0;32m--> 781\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    782\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    783\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMethod `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m` requires PyTorch.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniforge3/envs/meta/lib/python3.9/site-packages/transformers/training_args.py:1035\u001b[0m, in \u001b[0;36mTrainingArguments._setup_devices\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1031\u001b[0m \u001b[38;5;129m@cached_property\u001b[39m\n\u001b[1;32m   1032\u001b[0m \u001b[38;5;129m@torch_required\u001b[39m\n\u001b[1;32m   1033\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_setup_devices\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch.device\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   1034\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPyTorch: setting up devices\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1035\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdistributed\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_initialized\u001b[49m() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlocal_rank \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   1036\u001b[0m         logger\u001b[38;5;241m.\u001b[39mwarning(\n\u001b[1;32m   1037\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch.distributed process group is initialized, but local_rank == -1. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1038\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIn order to use Torch DDP, launch your script with `python -m torch.distributed.launch\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1039\u001b[0m         )\n\u001b[1;32m   1040\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mno_cuda:\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'torch.distributed' has no attribute 'is_initialized'"
     ]
    }
   ],
   "source": [
    "args = TrainingArguments('outputs', learning_rate=lr, warmup_ratio=0.1, lr_scheduler_type='cosine', fp16=True,\n",
    "    evaluation_strategy=\"epoch\", per_device_train_batch_size=bs, per_device_eval_batch_size=bs*2,\n",
    "    num_train_epochs=epochs, weight_decay=wd, report_to='none')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be071fdd",
   "metadata": {},
   "source": [
    "We can now create our model, and `Trainer` which is a class which combines the data and mdoel together (just like Learner in fastai)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "6225d4fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-v3-small were not used when initializing DebertaV2ForSequenceClassification: ['mask_predictions.dense.weight', 'mask_predictions.LayerNorm.weight', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.LayerNorm.weight', 'lm_predictions.lm_head.dense.weight', 'mask_predictions.classifier.weight', 'mask_predictions.dense.bias', 'mask_predictions.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.bias', 'mask_predictions.classifier.bias']\n",
      "- This IS expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-v3-small and are newly initialized: ['pooler.dense.bias', 'classifier.weight', 'pooler.dense.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'args' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [51]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m model \u001b[38;5;241m=\u001b[39m AutoModelForSequenceClassification\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_nm, num_labels\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(model, \u001b[43margs\u001b[49m, train_dataset\u001b[38;5;241m=\u001b[39mdds[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m], eval_dataset\u001b[38;5;241m=\u001b[39mdds[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m      3\u001b[0m                tokenizer\u001b[38;5;241m=\u001b[39mtokz, compute_metrics\u001b[38;5;241m=\u001b[39mcorr)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'args' is not defined"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(model_nm, num_labels=1)\n",
    "trainer = Trainer(model, args, train_dataset=dds['train'], eval_dataset=dds['test'],\n",
    "               tokenizer=tokz, compute_metrics=corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c711b84",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
