{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "14bcb552",
   "metadata": {},
   "source": [
    "# Huggingface datasets\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "949c4f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Most basic stuff for EDA.\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', 50)\n",
    "pd.set_option('display.max_rows', 150)\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "\n",
    "# !pip install transformers\n",
    "# !pip install datasets\n",
    "from datasets import load_dataset, load_metric\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer \n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a737b0a",
   "metadata": {},
   "source": [
    "# Read the data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcd366d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tweets = pd.read_csv('../data/train.csv')\n",
    "test_tweets = pd.read_csv('../data/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f0f950c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tweets.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e945beb3",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3af15c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style('whitegrid')\n",
    "sns.countplot(y=train_tweets['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8884b362",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tweets['location'].value_counts().head(n=20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f412867e",
   "metadata": {},
   "source": [
    "Let's clean the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "005204fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some basic helper functions to clean text by removing urls, emojis, html tags and punctuations.\n",
    "import re\n",
    "import string\n",
    "\n",
    "def remove_URL(text):\n",
    "    url = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    return url.sub(r'', text)\n",
    "\n",
    "\n",
    "def remove_emoji(text):\n",
    "    emoji_pattern = re.compile(\n",
    "        '['\n",
    "        u'\\U0001F600-\\U0001F64F'  # emoticons\n",
    "        u'\\U0001F300-\\U0001F5FF'  # symbols & pictographs\n",
    "        u'\\U0001F680-\\U0001F6FF'  # transport & map symbols\n",
    "        u'\\U0001F1E0-\\U0001F1FF'  # flags (iOS)\n",
    "        u'\\U00002702-\\U000027B0'\n",
    "        u'\\U000024C2-\\U0001F251'\n",
    "        ']+',\n",
    "        flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', text)\n",
    "\n",
    "\n",
    "def remove_html(text):\n",
    "    html = re.compile(r'<.*?>|&([a-z0-9]+|#[0-9]{1,6}|#x[0-9a-f]{1,6});')\n",
    "    return re.sub(html, '', text)\n",
    "\n",
    "\n",
    "def remove_punct(text):\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    return text.translate(table)\n",
    "\n",
    "# Applying helper functions on Train Dataset\n",
    "\n",
    "train_tweets['text_clean'] = train_tweets['text'].apply(lambda x: remove_URL(x))\n",
    "train_tweets['text_clean'] = train_tweets['text_clean'].apply(lambda x: remove_emoji(x))\n",
    "train_tweets['text_clean'] = train_tweets['text_clean'].apply(lambda x: remove_html(x))\n",
    "train_tweets['text_clean'] = train_tweets['text_clean'].apply(lambda x: remove_punct(x))\n",
    "\n",
    "# Applying helper functions on Test Dataset\n",
    "\n",
    "test_tweets['text_clean'] = test_tweets['text'].apply(lambda x: remove_URL(x))\n",
    "test_tweets['text_clean'] = test_tweets['text_clean'].apply(lambda x: remove_emoji(x))\n",
    "test_tweets['text_clean'] = test_tweets['text_clean'].apply(lambda x: remove_html(x))\n",
    "test_tweets['text_clean'] = test_tweets['text_clean'].apply(lambda x: remove_punct(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acf23a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tweets.fillna(\"\",inplace=True)\n",
    "test_tweets.fillna(\"\",inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7a5d0f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_nm = 'microsoft/deberta-v3-small'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c0a63e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokz = AutoTokenizer.from_pretrained(model_nm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "594c7102",
   "metadata": {},
   "outputs": [],
   "source": [
    "sep = tokz.sep_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33bc7b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tweets['train'] = train_tweets['text_clean'] + sep + train_tweets['location'] + sep + train_tweets['keyword']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81032e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tweets.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5f05d26",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4385eaf2",
   "metadata": {},
   "source": [
    "Time to import some stuff we'll need for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b9a45bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import warnings,transformers,logging,torch\n",
    "from transformers import TrainingArguments,Trainer\n",
    "from transformers import AutoModelForSequenceClassification,AutoTokenizer\n",
    "\n",
    "from datasets import load_dataset, Dataset, DatasetDict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a6b7379",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tok_func(x):\n",
    "    return tokz(x['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53d81266",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = Dataset.from_pandas(train_tweets)\n",
    "eval_ds = Dataset.from_pandas(test_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1606208",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tweets.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44fa3e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tok_ds = ( ds.map(tok_func, batched=True, remove_columns=['id', 'keyword', 'location', 'text', 'text_clean'])\n",
    "               .rename_column('target','label'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdf73815",
   "metadata": {},
   "outputs": [],
   "source": [
    "tok_ds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "013dc88e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dds = tok_ds.train_test_split(test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21b48780",
   "metadata": {},
   "source": [
    "# Initial model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86b96ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr,bs = 8e-5,32\n",
    "wd,epochs = 0.01,4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "816dc04c",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = TrainingArguments('outputs', learning_rate=lr, warmup_ratio=0.1, lr_scheduler_type='cosine', fp16=True,\n",
    "    evaluation_strategy=\"epoch\", per_device_train_batch_size=bs, per_device_eval_batch_size=bs*2,\n",
    "    num_train_epochs=epochs, weight_decay=wd, report_to='none')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be071fdd",
   "metadata": {},
   "source": [
    "We can now create our model, and `Trainer` which is a class which combines the data and mdoel together (just like Learner in fastai)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09db0f35",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_metric\n",
    "accuracy = load_metric(\"accuracy\")\n",
    "f1 = load_metric('f1')\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return {\n",
    "        'accuracy': accuracy.compute(predictions = predictions, references=labels)['accuracy'],\n",
    "        'f1': f1.compute(predictions = predictions, references=labels)['f1']\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6225d4fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(model_nm, num_labels=2)\n",
    "trainer = Trainer(model, args, train_dataset=dds['train'], eval_dataset=dds['test'],\n",
    "               tokenizer=tokz, compute_metrics=compute_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c711b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "#trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39a1e5c0",
   "metadata": {},
   "source": [
    "# New Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "549d370f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from transformers.modeling_outputs import SequenceClassifierOutput\n",
    "from transformers import AutoModel, AutoConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "140c192b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomModel(nn.Module):\n",
    "    def __init__(self, checkpoint, num_labels):\n",
    "        super(CustomModel, self).__init__()\n",
    "        self.num_labels = num_labels\n",
    "        \n",
    "        # Load a model with given checkpoint and extract its body\n",
    "        self.model  = AutoModel.from_pretrained(checkpoint,config=AutoConfig.from_pretrained(checkpoint, output_attentions=True,output_hidden_states=True))\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.classifier = nn.Linear(768, num_labels)\n",
    "        \n",
    "    def forward(self, input_ids = None, attention_mask = None, labels = None):\n",
    "        # Extract outputs from the body\n",
    "        outputs = self.model(input_ids = input_ids, attention_mask=attention_mask)\n",
    "        \n",
    "        # Adds a custom layer\n",
    "        sequence_output = self.dropout(outputs[0])\n",
    "        \n",
    "        logits = self.classifier(sequence_output[:,0,:].view(-1,768))\n",
    "        \n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss_fct = nn.CrossEntropyLoss()\n",
    "            loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "            \n",
    "\n",
    "        return SequenceClassifierOutput(loss = loss, logits = logits, hidden_states = outputs.hidden_states, attentions=outputs.attentions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d31093b",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_model = CustomModel(checkpoint = model_nm,num_labels=2)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_nm, num_labels=2)\n",
    "\n",
    "trainer = Trainer(custom_model, args, train_dataset=dds['train'], eval_dataset=dds['test'],\n",
    "               tokenizer=tokz, compute_metrics=compute_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6ef2be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#trainer.train(ignore_keys_for_eval=['hidden_states','attentions'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "045f6897",
   "metadata": {},
   "source": [
    "# LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a768e4da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pack_padded_sequence\n",
    "\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, checkpoint, num_labels):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.num_labels = num_labels\n",
    "        \n",
    "        # Load a model with given checkpoint and extract its body\n",
    "        self.model  = AutoModel.from_pretrained(checkpoint,config=AutoConfig.from_pretrained(checkpoint, output_attentions=True,output_hidden_states=True))\n",
    "        self.lstm_hidden_size = 768\n",
    "        self.lstm = nn.LSTM(768, self.lstm_hidden_size, bidirectional=True)\n",
    "        \n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.classifier = nn.Linear(2*self.lstm_hidden_size, num_labels)\n",
    "        \n",
    "    def forward(self, input_ids = None, attention_mask = None, labels = None):\n",
    "        # Extract outputs from the body\n",
    "        # get sentence length with pad_id = 0\n",
    "        sent_lengths = attention_mask.sum(dim=-1).cpu()\n",
    "        #sent_lengths = get_sent_lengths(input_ids)\n",
    "        outputs = self.model(input_ids = input_ids, attention_mask=attention_mask)\n",
    "        \n",
    "        \n",
    "        # pool the output using LSTM layer\n",
    "        \n",
    "        # sequence output is batch_size, seq_length, hidden_dim\n",
    "        enc_hiddens, (last_hidden, last_cell) = self.lstm(pack_padded_sequence(outputs[0], sent_lengths, enforce_sorted=False, batch_first=True))\n",
    "        \n",
    "        output_hidden = torch.cat((last_hidden[0], last_hidden[1]), dim=1)  # (batch_size, 2*hidden_size)\n",
    "        output_hidden = self.dropout(output_hidden)\n",
    "        \n",
    "        logits = self.classifier(output_hidden)\n",
    "        \n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss_fct = nn.CrossEntropyLoss()\n",
    "            loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "            \n",
    "\n",
    "        return SequenceClassifierOutput(loss = loss, logits = logits, hidden_states = outputs.hidden_states, attentions=outputs.attentions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "280dd128",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_model = LSTMModel(checkpoint = model_nm,num_labels=2)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_nm, num_labels=2)\n",
    "\n",
    "trainer = Trainer(custom_model, args, train_dataset=dds['train'], eval_dataset=dds['test'],\n",
    "               tokenizer=tokz, compute_metrics=compute_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "907fce8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train(ignore_keys_for_eval=['hidden_states','attentions'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81a7e5d9",
   "metadata": {},
   "source": [
    "# Eval dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2dd8d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = trainer.evaluate(eval_dataset = dds['test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77025ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fc9ffc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = trainer.predict(dds['test'],metric_key_prefix='predict').predictions\n",
    "predictions = np.argmax(predictions,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b441979",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "967ae8aa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "73f7923d63c6a0e3ec1cb33f7a655308159eba43e4be71ca4ac9906ec9b93272"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
